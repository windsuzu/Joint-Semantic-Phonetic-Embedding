{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 23653,
     "status": "ok",
     "timestamp": 1618318484460,
     "user": {
      "displayName": "NE6081022ÁéãÂ£´Êù∞",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "IDV7wZWMnqBZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import built-in Python libs\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "import unicodedata\n",
    "\n",
    "# Import data science libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import weights & bias\n",
    "import wandb\n",
    "\n",
    "# Import deep learning libs\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Import data preprocessing libs\n",
    "from tokenizers import Tokenizer, normalizers, pre_tokenizers, decoders, NormalizedString, PreTokenizedString\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.normalizers import NFKC\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import fasttext\n",
    "import jieba\n",
    "from janome.tokenizer import Tokenizer as jTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jLPQX3InOdG",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Split Raw Dataset to Source and Target datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 733,
     "status": "ok",
     "timestamp": 1618303481864,
     "user": {
      "displayName": "NE6081022ÁéãÂ£´Êù∞",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "Tm-NwXepumOs",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = Path.cwd().parent\n",
    "data_dir = root_dir / \"dataset\" / \"ASPEC-JC\"\n",
    "\n",
    "def extract_jp_ch_datasets(txt_arr):\n",
    "    p = np.array([list(map(str.strip, text.split(\"|||\")[1:])) for text in txt_arr])\n",
    "    jp = p[:, 0]\n",
    "    ch = p[:, 1]\n",
    "    return ch, jp\n",
    "\n",
    "\n",
    "def split_raw_text(txt_path):\n",
    "    with txt_path.open() as f:\n",
    "        ch, jp = extract_jp_ch_datasets(f.readlines())\n",
    "        \n",
    "        np.savetxt(txt_path.parent / \"ch.txt\", ch, fmt=\"%s\")\n",
    "        np.savetxt(txt_path.parent / \"jp.txt\", jp, fmt=\"%s\")\n",
    "\n",
    "\n",
    "dev_txt = data_dir / \"dev\" / \"dev.txt\"\n",
    "devtest_txt = data_dir / \"devtest\" / \"devtest.txt\"\n",
    "test_txt = data_dir / \"test\" / \"test.txt\"\n",
    "train_txt = data_dir / \"train\" / \"train.txt\"\n",
    "\n",
    "# uncomment to run the split program\n",
    "# split_raw_text(dev_txt)\n",
    "# split_raw_text(devtest_txt)\n",
    "# split_raw_text(test_txt)\n",
    "# split_raw_text(train_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zg9yMxFMnsjH"
   },
   "source": [
    "## Log Raw Data Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "executionInfo": {
     "elapsed": 7977,
     "status": "ok",
     "timestamp": 1618303499456,
     "user": {
      "displayName": "NE6081022ÁéãÂ£´Êù∞",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "I_GlWuyLnWtu",
    "outputId": "bfcae0ec-7029-4f10-a406-95717557a7bc",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.25<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">raw_data</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation/runs/2x0aragc\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation/runs/2x0aragc</a><br/>\n",
       "                Run data is saved locally in <code>/content/wandb/run-20210413_084456-2x0aragc</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='phonetic-translation', \n",
    "                 entity='windsuzu',\n",
    "                 group=\"dataset\",\n",
    "                 name=\"raw_data\",\n",
    "                 job_type=\"data_upload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 10893,
     "status": "ok",
     "timestamp": 1618303525088,
     "user": {
      "displayName": "NE6081022ÁéãÂ£´Êù∞",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "B5i2IKDro6dA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save raw txt file to artifact\n",
    "#\n",
    "# |-- train\n",
    "#     |-- ch & jp\n",
    "# |-- dev\n",
    "#     |-- ch & jp\n",
    "# |-- devtest\n",
    "#     |-- ch & jp\n",
    "# |-- test\n",
    "#     |-- ch & jp\n",
    "\n",
    "root_dir = Path.cwd().parent\n",
    "data_dir = root_dir / \"dataset\" / \"ASPEC-JC\"\n",
    "raw_data_types = [\"train\", \"dev\", \"devtest\", \"test\"]\n",
    "\n",
    "artifacts = {}\n",
    "\n",
    "for data_type in raw_data_types:\n",
    "    artifacts[data_type] = wandb.Artifact(data_type, \"raw_data\")\n",
    "    artifacts[data_type].add_file(data_dir / data_type / \"ch.txt\", \"ch.txt\")\n",
    "    artifacts[data_type].add_file(data_dir / data_type / \"jp.txt\", \"jp.txt\")\n",
    "\n",
    "for data_type, artifact in artifacts.items():\n",
    "    run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Corpus Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwindsuzu\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">honest-lion-135</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation/runs/2gpnjpav\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation/runs/2gpnjpav</a><br/>\n",
       "                Run data is saved locally in <code>/home/windsuzu/phonetics-in-chinese-japanese-machine-translation/experiments/main/wandb/run-20210501_171419-2gpnjpav</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='phonetic-translation', \n",
    "                 entity='windsuzu',\n",
    "                 group=\"dataset\",\n",
    "                 job_type=\"corpus_filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Downloading large artifact train:latest, 205.64MB. 2 files... Done. 0:0:0\n"
     ]
    }
   ],
   "source": [
    "# Download Raw Data\n",
    "train_data_art = run.use_artifact(\"train:latest\")\n",
    "train_data_dir = train_data_art.download()\n",
    "\n",
    "# Open Raw Data\n",
    "with open(Path(train_data_dir) / \"ch.txt\", encoding=\"utf8\") as f:\n",
    "    ch_docs = f.readlines()\n",
    "\n",
    "with open(Path(train_data_dir) / \"jp.txt\", encoding=\"utf8\") as f:\n",
    "    jp_docs = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672315"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_parallel_docs = list(zip(ch_docs, jp_docs))\n",
    "len(original_parallel_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Filtering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_MODEL_PATH = '../fasttext/lid.176.bin'\n",
    "model = fasttext.load_model(PRETRAINED_MODEL_PATH)\n",
    "\n",
    "\n",
    "def pass_length_ratio(ch, jp):\n",
    "    l_r = len(ch) / len(jp)\n",
    "    return l_r > 0.5 and l_r < 1.4\n",
    "\n",
    "\n",
    "def pass_length(ch, jp):\n",
    "    return len(ch) > 5 and len(ch) < 80 and len(jp) > 10 and len(jp) < 100\n",
    "\n",
    "\n",
    "def pass_same_sentence(ch, jp):\n",
    "    return ch != jp\n",
    "\n",
    "\n",
    "def pass_language_identification(ch, jp):\n",
    "    return model.predict(ch.strip())[0][0] == \"__label__zh\" and model.predict(jp.strip())[0][0] == \"__label__ja\"\n",
    "\n",
    "\n",
    "def pass_imbalance_word(text):\n",
    "    count = 0\n",
    "    for c in text.strip():\n",
    "        try:\n",
    "            # find the difference between (ch+jp) and (en+num+space)\n",
    "            if c.encode(\"ascii\").isalpha() or c.isdigit() or c.isspace():\n",
    "                count += 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    actual = len(text) - count\n",
    "    return actual > count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Corpus Filtering\n",
    "\n",
    "1. Âà™Èô§Èï∑Â∫¶ÊØî‰æãÈÅéÂ§ßÁöÑÂè•Â≠ê\n",
    "2. Âà™Èô§ÈÅéÁü≠ÊàñÈÅéÈï∑ÁöÑÂè•Â≠ê\n",
    "3. Âà™Èô§‰∏ÄÊ®£ÁöÑÂè•Â≠ê\n",
    "4. Âà™Èô§‰∏çËÉΩË¢´ fasttext identification model Ë™çÂá∫Ë™ûË®ÄÁöÑÂè•Â≠ê\n",
    "5. Âà™Èô§Ëã±Êñá„ÄÅÊï∏Â≠óÁ¨¶ËôüÂ§öÊñº‰∏≠Êó•ÊñáÁöÑÂè•Â≠ê\n",
    "6. Âà™Èô§ÊúâÂ§öÁ®ÆÂ∞çÊáâÁöÑÁøªË≠ØÁöÑÂè•Â≠ê\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557685"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered = []\n",
    "overlap = {}\n",
    "\n",
    "for ch, jp in original_parallel_docs:\n",
    "    # change full-width character to half-width\n",
    "    ch = unicodedata.normalize(\"NFKC\", ch)\n",
    "    jp = unicodedata.normalize(\"NFKC\", jp)\n",
    "    \n",
    "    if not pass_length_ratio(ch, jp):\n",
    "        continue\n",
    "    \n",
    "    if not pass_length(ch, jp):\n",
    "        continue\n",
    "    \n",
    "    if not pass_same_sentence(ch, jp):\n",
    "        continue\n",
    "    \n",
    "    if not pass_language_identification(ch, jp):\n",
    "        continue\n",
    "    \n",
    "    if not pass_imbalance_word(ch) or not pass_imbalance_word(jp):\n",
    "        continue\n",
    "    \n",
    "    if overlap.get(ch) or overlap.get(jp):\n",
    "        continue\n",
    "    else:\n",
    "        overlap[ch] = jp\n",
    "        overlap[jp] = ch\n",
    "        \n",
    "    filtered.append((ch, jp))\n",
    "\n",
    "len(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Alignment\n",
    "\n",
    "### Step 1\n",
    "\n",
    "ÊääÊâÄÊúâÁöÑ‰∏≠ÊñáÂíåÊó•ÊñáÂè•Â≠êÈÉΩÁî® jieba Âíå janome ÈÄ≤Ë°å tokenizationÔºå‰∏¶ÁµÑÂêàÊàêË¶Å‰∏üÈÄ≤ fast_align Ê®°ÂûãÁöÑÊ†ºÂºè„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "janome = jTokenizer()\n",
    "corpus = []\n",
    "\n",
    "for ch, jp in filtered:\n",
    "    ch_str_tokenized = \" \".join(jieba.cut(ch[:-1]))\n",
    "    jp_str_tokenized = \" \".join(janome.tokenize(jp[:-1], wakati=True))\n",
    "    line = f\"{ch_str_tokenized} ||| {jp_str_tokenized}\\n\"\n",
    "    corpus.append(line)\n",
    "\n",
    "with open(\"corpus.zh-ja\", 'w', encoding=\"utf8\") as f:\n",
    "    f.writelines(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Áî® fast_align Ë∑ëÂá∫ `forward.align` Âíå `reverse.align` Ë®àÁÆóÊØè‰∏ÄÂè•ÁöÑ word_alignment ÂàÜÊï∏„ÄÇ\n",
    "\n",
    "`align` Êñá‰ª∂ÁöÑÊØè‰∏ÄË°åÊ†ºÂºèÁÇ∫: `alignment ||| score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ./fast_align -i corpus/corpus.zh-ja -d -o -v -s > corpus/forward.align\n",
    "# ./fast_align -i corpus/corpus.zh-ja -d -o -v -s -r > corpus/reverse.align\n",
    "\n",
    "with open(\"../fast_align/corpus/forward.align\") as f:\n",
    "    forward = list(map(lambda x: float(x.split(\" ||| \")[1].strip()), f.readlines()))\n",
    "\n",
    "with open(\"../fast_align/corpus/reverse.align\") as f:\n",
    "    reverse = list(map(lambda x: float(x.split(\" ||| \")[1].strip()), f.readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "Âà™Èô§Ê©üÁéáÂ∞èÊñº -160 ÁöÑÂè•Â≠ê (ÂÖ± 96703 Âè•)ÔºåÁõÆÁöÑ‰∏ªË¶ÅÊòØÈôç‰ΩéË≥áÊñôÈõÜÂ§ßÂ∞è„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.90000e+01, 3.54000e+03, 9.31340e+04, 2.87882e+05, 1.73100e+05]),\n",
       " array([-395.466   , -317.093915, -238.72183 , -160.349745,  -81.97766 ,\n",
       "          -3.605575]),\n",
       " <BarContainer object of 5 artists>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alignment_scores = [sum(score) / 2 for score in zip(forward, reverse)]\n",
    "plt.hist(alignment_scores, bins=5)\n",
    "\n",
    "# -3 ~ -81     173100\n",
    "# -81 ~ -160   287882\n",
    "#\n",
    "# -160 ~ -238  93134\n",
    "# -238 ~ -317  3540\n",
    "# -317 ~ -395  29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_filtered = []\n",
    "for i, score in enumerate(alignment_scores):\n",
    "    if score > -161:\n",
    "        final_filtered.append(filtered[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462582"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = Path.cwd().parent\n",
    "data_dir = root_dir / \"dataset\" / \"ASPEC-JC\"\n",
    "\n",
    "ch = np.array(final_filtered)[:, 0]\n",
    "jp = np.array(final_filtered)[:, 1]\n",
    "\n",
    "np.savetxt(data_dir / \"train\" / \"filtered_ch.txt\", ch, fmt=\"%s\", newline=\"\", encoding=\"utf8\")\n",
    "np.savetxt(data_dir / \"train\" / \"filtered_jp.txt\", jp, fmt=\"%s\", newline=\"\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_artifacts.Artifact at 0x7fd4560da730>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_artifact = wandb.Artifact(\"filtered_train\", \"filtered_data\")\n",
    "\n",
    "filtered_artifact.add_file(data_dir / \"train\" / \"filtered_ch.txt\", \"ch.txt\")\n",
    "filtered_artifact.add_file(data_dir / \"train\" / \"filtered_jp.txt\", \"jp.txt\")\n",
    "\n",
    "run.log_artifact(filtered_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Sampling Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">blooming-field-208</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation/runs/2rzqb6y4\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation/runs/2rzqb6y4</a><br/>\n",
       "                Run data is saved locally in <code>D:\\Project\\phonetics-in-chinese-japanese-machine-translation\\experiments\\main\\wandb\\run-20210509_162925-2rzqb6y4</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='phonetic-translation', \n",
    "                 entity='windsuzu',\n",
    "                 group=\"dataset\",\n",
    "                 job_type=\"sampling_training_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Downloading large artifact filtered_train:latest, 99.47MB. 2 files... Done. 0:0:0\n"
     ]
    }
   ],
   "source": [
    "# Download Raw Data\n",
    "train_data_art = run.use_artifact(\"filtered_train:latest\")\n",
    "train_data_dir = train_data_art.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path.cwd().parent\n",
    "data_dir = root_dir / \"dataset\" / \"ASPEC-JC\" / \"train\"\n",
    "\n",
    "sampled_ch_dir = data_dir / \"sampled_ch.txt\"\n",
    "sampled_jp_dir = data_dir / \"sampled_jp.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(train_data_dir) / \"ch.txt\", encoding=\"utf8\") as f:\n",
    "    ch_data = f.readlines()\n",
    "\n",
    "with open(Path(train_data_dir) / \"jp.txt\", encoding=\"utf8\") as f:\n",
    "    jp_data = f.readlines()\n",
    "    \n",
    "data = list(zip(ch_data, jp_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = random.sample(data, 50000)\n",
    "ch_sampled, jp_sampled = zip(*sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sampled_ch_dir, 'w', encoding=\"utf8\") as f:\n",
    "    for txt in ch_sampled:\n",
    "        f.write(txt)\n",
    "\n",
    "with open(sampled_jp_dir, 'w', encoding=\"utf8\") as f:\n",
    "    for txt in jp_sampled:\n",
    "        f.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_artifacts.Artifact at 0x23ae17f72e0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_artifact = wandb.Artifact(\"sampled_train\", \"sampled_data\")\n",
    "\n",
    "filtered_artifact.add_file(data_dir / \"sampled_ch.txt\", \"ch.txt\")\n",
    "filtered_artifact.add_file(data_dir / \"sampled_jp.txt\", \"jp.txt\")\n",
    "\n",
    "run.log_artifact(filtered_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Tokenization Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_tokenizer(\n",
    "    tokenizer,\n",
    "    files,\n",
    "    unk_token=\"[UNK]\",\n",
    "    vocab_size=32000,\n",
    "    min_frequency=2,\n",
    "):\n",
    "    trainer = BpeTrainer(\n",
    "        special_tokens=[unk_token, \"[BOS]\", \"[EOS]\", \"[PAD]\"],\n",
    "        vocab_size=vocab_size,\n",
    "        show_prorgess=True,\n",
    "        min_frequency=min_frequency,\n",
    "    )\n",
    "\n",
    "    if isinstance(files, str):\n",
    "        files = [files]\n",
    "\n",
    "    tokenizer.train(files, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_tokenizer(\n",
    "    tokenizer,\n",
    "    train_text_path: Path,\n",
    "    min_frequency=2,\n",
    "    padded=True,\n",
    "    post_process=True,\n",
    "):\n",
    "    assert train_text_path.exists(), \"Training Raw Text does not exist.\"\n",
    "\n",
    "    # Train tokenizer\n",
    "    train_tokenizer(tokenizer, str(train_text_path), min_frequency=min_frequency)\n",
    "\n",
    "    # Enable Padding\n",
    "    if padded:\n",
    "        tokenizer.enable_padding(\n",
    "            pad_id=tokenizer.token_to_id(\"[PAD]\"), pad_token=\"[PAD]\"\n",
    "        )\n",
    "\n",
    "    if post_process:\n",
    "        # Encode => BOS + sentence + EOS\n",
    "        tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"[BOS] $A [EOS]\",\n",
    "            special_tokens=[\n",
    "                (\"[BOS]\", tokenizer.token_to_id(\"[BOS]\")),\n",
    "                (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\")),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_tokenzier(tokenizer, save_path, has_custom_norm=False, has_custom_pretok=False, has_custom_dec=False):\n",
    "    if has_custom_norm:\n",
    "        tokenizer.normalizer = normalizers.NFKC()\n",
    "    \n",
    "    if has_custom_pretok:\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    \n",
    "    if has_custom_dec:\n",
    "        tokenizer.decoder = decoders.WordPiece()\n",
    "    \n",
    "    tokenizer.save(str(save_path))\n",
    "    \n",
    "    \n",
    "def load_tokenizer(tokenizer_path, custom_norm=None, custom_pretok=None, custom_dec=None):\n",
    "    tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    if custom_norm:\n",
    "        tokenizer.normalizer = custom_norm\n",
    "    \n",
    "    if custom_pretok:\n",
    "        tokenizer.pre_tokenizer = custom_pretok\n",
    "    \n",
    "    if custom_dec:\n",
    "        tokenizer.decoder = custom_dec\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUpMP-eOpnLv",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Google Sentencepiece Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306,
     "referenced_widgets": [
      "68b8a22147e642cb9ff32404a6f5df73",
      "ba8723ef855e46a9a489b50225bd1acb",
      "c14f1062d03c447a93a6cf85d944e093",
      "e87be027a35141a590e8173b3c2895f0",
      "af90fbddd11b43518d019073b56cb6d9",
      "1c164ba062484ac795b19c71632a6652",
      "3a34f0eeb71c4197b4c232afe2bf3162",
      "5890ac78442d4427bf7e74b26fa68b3f"
     ]
    },
    "executionInfo": {
     "elapsed": 6097,
     "status": "ok",
     "timestamp": 1618305901658,
     "user": {
      "displayName": "NE6081022ÁéãÂ£´Êù∞",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "yjlnnAp1qBEJ",
    "outputId": "5c670c8f-c4ca-4ba5-ca74-93295f4eb977",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: windsuzu (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": "\n                Tracking run with wandb version 0.10.30<br/>\n                Syncing run <strong style=\"color:#cdcd00\">sentence_piece</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation</a><br/>\n                Run page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation/runs/3renoogk\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation/runs/3renoogk</a><br/>\n                Run data is saved locally in <code>d:\\Project\\phonetics-in-chinese-japanese-machine-translation\\experiments\\main\\wandb\\run-20210510_001540-3renoogk</code><br/><br/>\n            ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='phonetic-translation', \n",
    "                 entity='windsuzu',\n",
    "                 group=\"tokenizer\",\n",
    "                 name=\"sentence_piece\",\n",
    "                 job_type=\"build_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1438,
     "status": "ok",
     "timestamp": 1618305904606,
     "user": {
      "displayName": "NE6081022ÁéãÂ£´Êù∞",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "lpd_nM0khGat",
    "outputId": "9bc5ffb5-82e3-4762-9db9-274835071347",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Downloading large artifact train:latest, 205.64MB. 2 files... Done. 0:0:0\n"
     ]
    }
   ],
   "source": [
    "# Download Raw Data\r\n",
    "train_data_art = run.use_artifact(\"train:latest\")\r\n",
    "train_data_dir = train_data_art.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1618305906923,
     "user": {
      "displayName": "NE6081022ÁéãÂ£´Êù∞",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "M19hCES8qU3l",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sentence_piece_tokenizer(unk_token=\"[UNK]\", dropout: float = None):\n",
    "    tokenizer = Tokenizer(BPE(dropout=dropout, unk_token=unk_token))\n",
    "    tokenizer.normalizer = NFKC()\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n",
    "        [\n",
    "            pre_tokenizers.Whitespace(),\n",
    "            pre_tokenizers.Punctuation(),\n",
    "            pre_tokenizers.Digits(),\n",
    "            pre_tokenizers.Metaspace(replacement=\"_\", add_prefix_space=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    tokenizer.decoder = decoders.Metaspace(replacement=\"_\", add_prefix_space=True)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 896,
     "status": "ok",
     "timestamp": 1618305907270,
     "user": {
      "displayName": "NE6081022ÁéãÂ£´Êù∞",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "3Im8LFxeqy98",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Tokenizer.json File Path\n",
    "\n",
    "root_dir = Path.cwd().parent\n",
    "tokenizer_dir = root_dir / \"tokenizer\"\n",
    "tokenizer_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ch_tokenizer_dir = tokenizer_dir / \"tokenizer_sentencepiece_ch.json\"\n",
    "jp_tokenizer_dir = tokenizer_dir / \"tokenizer_sentencepiece_jp.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 680,
     "status": "ok",
     "timestamp": 1618305907906,
     "user": {
      "displayName": "NE6081022ÁéãÂ£´Êù∞",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "SlGsMPhe-sao",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ch_tokenizer = build_tokenizer(\n",
    "    sentence_piece_tokenizer(),\n",
    "    Path(train_data_dir) / \"ch.txt\",\n",
    "    min_frequency=2,\n",
    ")\n",
    "\n",
    "jp_tokenizer = build_tokenizer(\n",
    "    sentence_piece_tokenizer(),\n",
    "    Path(train_data_dir) / \"jp.txt\",\n",
    "    min_frequency=2,\n",
    ")\n",
    "\n",
    "save_tokenzier(ch_tokenizer, ch_tokenizer_dir)\n",
    "save_tokenzier(jp_tokenizer, jp_tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ch_tokenizer = load_tokenizer(ch_tokenizer_dir)\n",
    "jp_tokenizer = load_tokenizer(jp_tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1618305908555,
     "user": {
      "displayName": "NE6081022ÁéãÂ£´Êù∞",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "meA6U3Gxh30s",
    "outputId": "dfa83d74-9df6-4469-98d4-f71109b76ef0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Êè¥Âä©', 13444), ('_VI', 15679), ('ÊòØÈùû', 16303), ('57', 29055), ('IEEE', 19296), ('pper', 27215), ('_high', 22774), ('‰∏ãÂéª', 14066), ('Âå∫Âùó', 17190), ('ÊºÇÊµÆ', 19630)]\n",
      "32000\n",
      "\n",
      "[1, 7919, 4351, 972, 6035, 6337, 21410, 3863, 3842, 7190, 13212, 12959, 1616, 6031, 6080, 6043, 6039, 7732, 6513, 6043, 6039, 7670, 6086, 6031, 6037, 6337, 21410, 9092, 7621, 1413, 6031, 6080, 6043, 6039, 9096, 6513, 6043, 6039, 7029, 6086, 6032, 66, 0, 2]\n",
      "['[BOS]', '_‰∏ª', 'Ëåé', 'Âèä', '_1', '_Ê¨°', 'ÂàÜËòñ', 'Á≤æ', 'Á±≥', 'ËõãÁôΩË¥®', 'Âê´ÈáèÁöÑ', 'Ê†áÂáÜÂÅèÂ∑Æ', 'Â∞è', '_,', '_‰∏∫', '_0', '_.', '_28', '_~', '_0', '_.', '_35', '_%', '_,', '_2', '_Ê¨°', 'ÂàÜËòñ', 'ÁöÑÊ†áÂáÜ', 'ÂÅèÂ∑Æ', 'Â§ß', '_,', '_‰∏∫', '_0', '_.', '_44', '_~', '_0', '_.', '_60', '_%', '_„ÄÇ', '_', '[UNK]', '[EOS]']\n"
     ]
    },
    {
     "data": {
      "text/plain": "'‰∏ªËåéÂèä 1 Ê¨°ÂàÜËòñÁ≤æÁ±≥ËõãÁôΩË¥®Âê´ÈáèÁöÑÊ†áÂáÜÂÅèÂ∑ÆÂ∞è , ‰∏∫ 0 . 28 ~ 0 . 35 % , 2 Ê¨°ÂàÜËòñÁöÑÊ†áÂáÜÂÅèÂ∑ÆÂ§ß , ‰∏∫ 0 . 44 ~ 0 . 60 % „ÄÇ '"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print first ten vocab\n",
    "print([(key, val) for key, val in ch_tokenizer.get_vocab().items()][:10])\n",
    "print(ch_tokenizer.get_vocab_size())\n",
    "print()\n",
    "\n",
    "# Encode and Decode Testing\n",
    "encoded = ch_tokenizer.encode(\"‰∏ªËåéÂèä1Ê¨°ÂàÜËòñÁ≤æÁ±≥ËõãÁôΩË¥®Âê´ÈáèÁöÑÊ†áÂáÜÂÅèÂ∑ÆÂ∞è,‰∏∫0.28~0.35%,2Ê¨°ÂàÜËòñÁöÑÊ†áÂáÜÂÅèÂ∑ÆÂ§ß,‰∏∫0.44~0.60%„ÄÇüòÄ\")\n",
    "\n",
    "print(encoded.ids)\n",
    "print(encoded.tokens)\n",
    "ch_tokenizer.decode(encoded.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 624,
     "status": "ok",
     "timestamp": 1618305909068,
     "user": {
      "displayName": "NE6081022ÁéãÂ£´Êù∞",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "sFQlvvVIDyqK",
    "outputId": "477601f8-0222-48be-9be8-f96b74afd80d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_ÁΩ≤Âêç', 10462), ('„ÅÆÊ±∫ÂÆö', 12723), ('„ÇíË°®„Åô', 5211), ('_„Çπ„É´', 27450), ('_Áä∂ÊÖãÈÅ∑Áßª', 15217), ('ÂØõÂÆπ', 28862), ('Èºà', 4211), ('_RF', 11939), ('_„Å©„Å°', 9669), ('ÂóúÂ•Ω', 12379)]\n",
      "32000\n",
      "\n",
      "[1, 4334, 5828, 4487, 4492, 5580, 26658, 769, 2, 3, 3, 3, 3]\n",
      "['[BOS]', '_C', '_&', '_D', 'ÁÆ°ÁêÜ', 'ÊñΩË®≠', '„ÅÆÈ´òÂ∫¶', 'Âåñ', '[EOS]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "C & DÁÆ°ÁêÜÊñΩË®≠„ÅÆÈ´òÂ∫¶Âåñ\n",
      "\n",
      "[1, 64, 0, 10084, 2017, 2771, 4544, 4679, 5016, 382, 8756, 769, 2]\n",
      "['[BOS]', '_', '[UNK]', '_Áï∞', 'Ê•≠', 'Á®Æ', '„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ', '„Åã„Çâ„ÅÆ', 'Âú∞Âüü', '„Éñ', '„É©„É≥„Éâ', 'Âåñ', '[EOS]']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " Áï∞Ê•≠Á®Æ„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Åã„Çâ„ÅÆÂú∞Âüü„Éñ„É©„É≥„ÉâÂåñ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print first ten vocab\n",
    "print([(key, val) for key, val in jp_tokenizer.get_vocab().items()][:10])\n",
    "print(jp_tokenizer.get_vocab_size())\n",
    "print()\n",
    "\n",
    "# Encode and Decode Testing\n",
    "encoded = jp_tokenizer.encode_batch([\"Ôº£ÔºÜÔº§ÁÆ°ÁêÜÊñΩË®≠„ÅÆÈ´òÂ∫¶Âåñ\", \"üòÄÁï∞Ê•≠Á®Æ„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Åã„Çâ„ÅÆÂú∞Âüü„Éñ„É©„É≥„ÉâÂåñ\"])\n",
    "\n",
    "for i in range(2):\n",
    "    print(encoded[i].ids)\n",
    "    print(encoded[i].tokens)\n",
    "    print(encoded[i].attention_mask)\n",
    "    print(jp_tokenizer.decode(encoded[i].ids))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHvXQUoBRmDi"
   },
   "source": [
    "## Log Tokenizer Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1399,
     "status": "ok",
     "timestamp": 1618305911515,
     "user": {
      "displayName": "NE6081022ÁéãÂ£´Êù∞",
      "photoUrl": "",
      "userId": "16413260345102275028"
     },
     "user_tz": -480
    },
    "id": "zo-Y3hXjRbSC",
    "outputId": "6de0d4c6-e51e-4a9b-83f9-e2c87dcce7b7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<wandb.sdk.wandb_artifacts.Artifact at 0x22ec78bd250>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact = wandb.Artifact(\"sentencepiece\", \n",
    "                          type=\"tokenizer\",\n",
    "                          metadata={\"vocab\": 32000, \n",
    "                                    \"method\": \"SentencePiece\",\n",
    "                                    \"min_frequency\": 2})\n",
    "\n",
    "artifact.add_file(ch_tokenizer_dir, \"ch_tokenizer.json\")\n",
    "artifact.add_file(jp_tokenizer_dir, \"jp_tokenizer.json\")\n",
    "run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Specific Tokenization (Jieba & Janome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": "\n                Tracking run with wandb version 0.10.30<br/>\n                Syncing run <strong style=\"color:#cdcd00\">language_specific</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation</a><br/>\n                Run page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation/runs/2s92vxch\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation/runs/2s92vxch</a><br/>\n                Run data is saved locally in <code>d:\\Project\\phonetics-in-chinese-japanese-machine-translation\\experiments\\main\\wandb\\run-20210510_002752-2s92vxch</code><br/><br/>\n            ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='phonetic-translation', \n",
    "                 entity='windsuzu',\n",
    "                 group=\"tokenizer\",\n",
    "                 name=\"language_specific\",\n",
    "                 job_type=\"build_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Downloading large artifact train:latest, 205.64MB. 2 files... Done. 0:0:0\n"
     ]
    }
   ],
   "source": [
    "# Download Raw Data\r\n",
    "train_data_art = run.use_artifact(\"train:latest\")\r\n",
    "train_data_dir = train_data_art.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Tokenizer.json File Path\n",
    "\n",
    "root_dir = Path.cwd().parent\n",
    "tokenizer_dir = root_dir / \"tokenizer\"\n",
    "tokenizer_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "jieba_tokenizer_dir = tokenizer_dir / \"tokenizer_jieba.json\"\n",
    "janome_tokenizer_dir = tokenizer_dir / \"tokenizer_janome.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class JiebaPreTokenizer:\r\n",
    "    def jieba_split(self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\r\n",
    "        splits = []\r\n",
    "        for _, start, stop in jieba.tokenize(str(normalized_string)):\r\n",
    "            splits.append(normalized_string[start:stop])\r\n",
    "        return splits\r\n",
    "    \r\n",
    "    def pre_tokenize(self, pretok: PreTokenizedString):\r\n",
    "         pretok.split(self.jieba_split)\r\n",
    "            \r\n",
    "            \r\n",
    "class JiebaDecoder:\r\n",
    "    def decode(self, tokens: List[str]) -> str:\r\n",
    "        return \"\".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_jieba_tokenizer(unk_token=\"[UNK]\", dropout: float = None):\n",
    "    tokenizer = Tokenizer(BPE(dropout=dropout, unk_token=unk_token))\n",
    "    tokenizer.normalizer = NFKC()\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.PreTokenizer.custom(JiebaPreTokenizer())\n",
    "    tokenizer.decoder = decoders.Decoder.custom(JiebaDecoder())\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Jay\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.768 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "jieba_tokenizer = build_tokenizer(\r\n",
    "    create_jieba_tokenizer(),\r\n",
    "    Path(train_data_dir) / \"ch.txt\",\r\n",
    "    min_frequency=2,\r\n",
    ")\r\n",
    "\r\n",
    "save_tokenzier(jieba_tokenizer, jieba_tokenizer_dir, has_custom_pretok=True, has_custom_dec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jieba_tokenizer = load_tokenizer(jieba_tokenizer_dir,\n",
    "                                 custom_pretok=pre_tokenizers.PreTokenizer.custom(JiebaPreTokenizer()),\n",
    "                                 custom_dec=decoders.Decoder.custom(JiebaDecoder()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ÊïÖÈöúËØäÊñ≠', 25298), ('‰∏úÂçó‰∫ö', 21529), ('Ë°åËøõ', 16269), ('È∏™', 5927), ('Schab', 31826), ('Ë¥üÂÄº', 15970), ('ÊûÅÂ∞è', 9831), ('ÊëòÈô§ÊúØ', 23108), ('TW', 18916), ('Â¶ñ', 1461)]\n",
      "32000\n",
      "\n",
      "[1, 15169, 974, 22, 2717, 14680, 3865, 3844, 6907, 6820, 3483, 9438, 1618, 17, 0, 2, 3, 3, 3, 3, 3, 3]\n",
      "['[BOS]', '‰∏ªËåé', 'Âèä', '1', 'Ê¨°', 'ÂàÜËòñ', 'Á≤æ', 'Á±≥', 'ËõãÁôΩË¥®', 'Âê´Èáè', 'ÁöÑ', 'Ê†áÂáÜÂÅèÂ∑Æ', 'Â∞è', ',', '[UNK]', '[EOS]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "‰∏ªËåéÂèä1Ê¨°ÂàÜËòñÁ≤æÁ±≥ËõãÁôΩË¥®Âê´ÈáèÁöÑÊ†áÂáÜÂÅèÂ∑ÆÂ∞è,\n",
      "\n",
      "[1, 462, 22648, 99, 9492, 6636, 17, 23, 2717, 14680, 3483, 9438, 1415, 17, 462, 6109, 8815, 99, 8789, 6567, 252, 2]\n",
      "['[BOS]', '‰∏∫', '0.28', '~', '0.3', '5%', ',', '2', 'Ê¨°', 'ÂàÜËòñ', 'ÁöÑ', 'Ê†áÂáÜÂÅèÂ∑Æ', 'Â§ß', ',', '‰∏∫', '0.', '44', '~', '0.6', '0%', '„ÄÇ', '[EOS]']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "‰∏∫0.28~0.35%,2Ê¨°ÂàÜËòñÁöÑÊ†áÂáÜÂÅèÂ∑ÆÂ§ß,‰∏∫0.44~0.60%„ÄÇ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print first ten vocab\n",
    "print([(key, val) for key, val in jieba_tokenizer.get_vocab().items()][:10])\n",
    "print(jieba_tokenizer.get_vocab_size())\n",
    "print()\n",
    "\n",
    "# Encode and Decode Testing\n",
    "encoded = jieba_tokenizer.encode_batch([\"‰∏ªËåéÂèä1Ê¨°ÂàÜËòñÁ≤æÁ±≥ËõãÁôΩË¥®Âê´ÈáèÁöÑÊ†áÂáÜÂÅèÂ∑ÆÂ∞è,üòÄ\", \"‰∏∫0.28~0.35%,2Ê¨°ÂàÜËòñÁöÑÊ†áÂáÜÂÅèÂ∑ÆÂ§ß,‰∏∫0.44~0.60%„ÄÇ\"])\n",
    "\n",
    "for i in range(2):\n",
    "    print(encoded[i].ids)\n",
    "    print(encoded[i].tokens)\n",
    "    print(encoded[i].attention_mask)\n",
    "    print(jieba_tokenizer.decode(encoded[i].ids))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Janome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ja_tokenizer = jTokenizer()\n",
    "class JanomePreTokenizer:\n",
    "    def janome_split(self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
    "        splits = []\n",
    "        i = 0\n",
    "        for token in ja_tokenizer.tokenize(str(normalized_string).strip(), wakati=True):\n",
    "            splits.append(normalized_string[i: i+len(token)])\n",
    "            i += len(token)\n",
    "        return splits\n",
    "    \n",
    "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
    "        pretok.split(self.janome_split)\n",
    "            \n",
    "            \n",
    "class JanomeDecoder:\n",
    "    def decode(self, tokens: List[str]) -> str:\n",
    "        return \"\".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_janome_tokenizer(unk_token=\"[UNK]\", dropout: float = None):\n",
    "    tokenizer = Tokenizer(BPE(dropout=dropout, unk_token=unk_token))\n",
    "    tokenizer.normalizer = NFKC()\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n",
    "        [\n",
    "            pre_tokenizers.Whitespace(),\n",
    "            pre_tokenizers.PreTokenizer.custom(JanomePreTokenizer()),\n",
    "        ]\n",
    "    )\n",
    "    tokenizer.decoder = decoders.Decoder.custom(JanomeDecoder())\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "janome_tokenizer = build_tokenizer(\r\n",
    "    create_janome_tokenizer(),\r\n",
    "    Path(train_data_dir) / \"jp.txt\",\r\n",
    "    min_frequency=2,\r\n",
    ")\r\n",
    "\r\n",
    "save_tokenzier(janome_tokenizer, janome_tokenizer_dir, has_custom_pretok=True, has_custom_dec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exception upon attempting to load a Tokenizer from file.\n",
    "# https://github.com/huggingface/tokenizers/issues/566\n",
    "\n",
    "janome_tokenizer = load_tokenizer(\n",
    "    janome_tokenizer_dir,\n",
    "    custom_pretok=pre_tokenizers.Sequence(\n",
    "        [\n",
    "            pre_tokenizers.Whitespace(),\n",
    "            pre_tokenizers.PreTokenizer.custom(JanomePreTokenizer()),\n",
    "        ]\n",
    "    ),\n",
    "    custom_dec=decoders.Decoder.custom(JanomeDecoder()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ë™øÊï¥', 5339), ('chi', 13329), ('ÂçîÂäõ', 6428), ('„Éó„É≠„Éù„Éï„Ç©„Éº„É´', 8448), ('Ë•ñ', 3453), ('„Ç∑„ÉÉ„Éó', 10591), ('„Ç≥„É≥„Éó„É™', 22386), ('„ÅÑ„Å£„Å±„ÅÑ', 28337), ('Èöß', 3974), ('May', 23591)]\n",
      "32000\n",
      "\n",
      "[1, 4802, 6713, 291, 14, 436, 3212, 4305, 18, 2072, 698, 262, 280, 283, 17, 15, 5794, 245, 17, 15, 5736, 7, 284, 5352, 0, 2]\n",
      "['[BOS]', 'Ê®ôÊ∫ñ', 'ÂÅèÂ∑Æ', '„ÅØ', ',', '‰∏ª', 'Ëåé', '„Åä„Çà„Å≥', '1', 'Ê¨°', 'ÂàÜ', '„Åí', '„Å§', '„Åß', '0', '.', '28', '„Äú', '0', '.', '35', '%', '„Å®', 'Â∞è„Åï„Åè', '[UNK]', '[EOS]']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Ê®ôÊ∫ñÂÅèÂ∑Æ„ÅØ,‰∏ªËåé„Åä„Çà„Å≥1Ê¨°ÂàÜ„Åí„Å§„Åß0.28„Äú0.35%„Å®Â∞è„Åï„Åè\n",
      "\n",
      "[1, 19, 2072, 698, 262, 280, 283, 17, 15, 6898, 245, 17, 15, 5023, 7, 284, 9089, 275, 227, 2, 3, 3, 3, 3, 3, 3]\n",
      "['[BOS]', '2', 'Ê¨°', 'ÂàÜ', '„Åí', '„Å§', '„Åß', '0', '.', '44', '„Äú', '0', '.', '60', '%', '„Å®', 'Â§ß„Åç„Åã„Å£', '„Åü', '„ÄÇ', '[EOS]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "2Ê¨°ÂàÜ„Åí„Å§„Åß0.44„Äú0.60%„Å®Â§ß„Åç„Åã„Å£„Åü„ÄÇ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print first ten vocab\n",
    "print([(key, val) for key, val in janome_tokenizer.get_vocab().items()][:10])\n",
    "print(janome_tokenizer.get_vocab_size())\n",
    "print()\n",
    "\n",
    "# Encode and Decode Testing\n",
    "encoded = janome_tokenizer.encode_batch([\"Ê®ôÊ∫ñÂÅèÂ∑Æ„ÅØ,‰∏ªËåé„Åä„Çà„Å≥1Ê¨°ÂàÜ„Åí„Å§„Åß0.28„Äú0.35%„Å®Â∞è„Åï„ÅèüòÄ\", \"2Ê¨°ÂàÜ„Åí„Å§„Åß0.44„Äú0.60%„Å®Â§ß„Åç„Åã„Å£„Åü„ÄÇ\"])\n",
    "\n",
    "for i in range(2):\n",
    "    print(encoded[i].ids)\n",
    "    print(encoded[i].tokens)\n",
    "    print(encoded[i].attention_mask)\n",
    "    print(janome_tokenizer.decode(encoded[i].ids))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Tokenizer Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<wandb.sdk.wandb_artifacts.Artifact at 0x22f17b3ca00>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact = wandb.Artifact(\"language_specific\",\r\n",
    "                          type=\"tokenizer\",\r\n",
    "                          metadata={\"vocab\": 32000, \r\n",
    "                                    \"method\": \"jieba&janome\",\r\n",
    "                                    \"min_frequency\": 2})\r\n",
    "\r\n",
    "artifact.add_file(jieba_tokenizer_dir, \"jieba_tokenizer.json\")\r\n",
    "artifact.add_file(janome_tokenizer_dir, \"janome_tokenizer.json\")\r\n",
    "run.log_artifact(artifact)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNfjeD/O41ga4aGmZW502GJ",
   "collapsed_sections": [],
   "mount_file_id": "1L02Qk-Dm-jqIakxj9MCrbk4fmL4qkjaN",
   "name": "ASPEC_JC_preprocess.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "910af126f78e4f70975a50f5d0344a29878143e0b01cc32c99ca6cf65dbefcc1"
   }
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1c164ba062484ac795b19c71632a6652": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a34f0eeb71c4197b4c232afe2bf3162": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5890ac78442d4427bf7e74b26fa68b3f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68b8a22147e642cb9ff32404a6f5df73": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c14f1062d03c447a93a6cf85d944e093",
       "IPY_MODEL_e87be027a35141a590e8173b3c2895f0"
      ],
      "layout": "IPY_MODEL_ba8723ef855e46a9a489b50225bd1acb"
     }
    },
    "af90fbddd11b43518d019073b56cb6d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba8723ef855e46a9a489b50225bd1acb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c14f1062d03c447a93a6cf85d944e093": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c164ba062484ac795b19c71632a6652",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_af90fbddd11b43518d019073b56cb6d9",
      "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r"
     }
    },
    "e87be027a35141a590e8173b3c2895f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5890ac78442d4427bf7e74b26fa68b3f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3a34f0eeb71c4197b4c232afe2bf3162",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}