{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c56feca-f3f1-4836-b6a2-e029635a2c92",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d631450-d119-445a-bad1-0b6ad00e51d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import built-in Python libs\n",
    "import pickle\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# Import data science libs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import deep learning libs\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import weights & bias\n",
    "import wandb\n",
    "\n",
    "# Import data preprocessing libs\n",
    "from tokenizers import Tokenizer, decoders, pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import NFKC\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0336920f-8cbe-480f-8321-d8bc7a5b8e7f",
   "metadata": {},
   "source": [
    "# Download Datasets, Tokenizers, DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fa478c9-58c0-4f9e-9d35-5923b0016715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwindsuzu\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.27<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">rnn_attention</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation/runs/30i663cw\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation/runs/30i663cw</a><br/>\n",
       "                Run data is saved locally in <code>/home/ai2019/ne6081022/project/phonetics-in-chinese-japanese-machine-translation/experiments/main/wandb/run-20210427_085121-30i663cw</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=\"phonetic-translation\",\n",
    "    entity=\"windsuzu\",\n",
    "    group=\"experiments\",\n",
    "    name=\"rnn_attention\",\n",
    "    job_type=\"baseline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ffe06b-484a-45ee-b7ae-9b41ce130e3e",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c12b40e-53c2-42ee-b5af-11d3c0548530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact train:latest, 205.64MB. 2 files... Done. 0:0:0\n"
     ]
    }
   ],
   "source": [
    "train_data_art = run.use_artifact(\"train:latest\")\n",
    "train_data_dir = train_data_art.download()\n",
    "\n",
    "dev_data_art = run.use_artifact(\"dev:latest\")\n",
    "dev_data_dir = dev_data_art.download()\n",
    "\n",
    "test_data_art = run.use_artifact(\"test:latest\")\n",
    "test_data_dir = test_data_art.download()\n",
    "\n",
    "data_dir = {\n",
    "    \"train\": train_data_dir,\n",
    "    \"dev\": dev_data_dir,\n",
    "    \"test\": test_data_dir,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713a1bde-d82e-40c4-bbbc-8a736cbd6503",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9331de4f-da3e-44a3-a3dd-641f79e2b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencepiece_tokenizer_art = run.use_artifact(\"sentencepiece:latest\")\n",
    "sentencepiece_tokenizer_dir = sentencepiece_tokenizer_art.download()\n",
    "ch_tokenizer_dir = Path(sentencepiece_tokenizer_dir) / \"ch_tokenizer.json\"\n",
    "jp_tokenizer_dir = Path(sentencepiece_tokenizer_dir) / \"jp_tokenizer.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f61436-b58e-4db8-b9a2-7eec275d5a21",
   "metadata": {},
   "source": [
    "## DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9261e259-5232-4d71-b440-6bd04a9da803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentencePieceDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        src_tokenizer_dir,\n",
    "        trg_tokenizer_dir,\n",
    "        batch_size=128,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.src_tokenizer_dir = src_tokenizer_dir\n",
    "        self.trg_tokenizer_dir = trg_tokenizer_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.src_tokenizer = self._load_tokenizer(self.src_tokenizer_dir)\n",
    "        self.trg_tokenizer = self._load_tokenizer(self.trg_tokenizer_dir)\n",
    "\n",
    "        if stage == \"fit\":\n",
    "            self.train_set = self._data_preprocess(self.data_dir[\"train\"])\n",
    "            self.val_set = self._data_preprocess(self.data_dir[\"dev\"])\n",
    "\n",
    "        if stage == \"test\":\n",
    "            self.test_set = self._data_preprocess(self.data_dir[\"test\"])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_set,\n",
    "            self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self._data_batching_fn,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_set,\n",
    "            self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self._data_batching_fn,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_set,\n",
    "            self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self._data_batching_fn,\n",
    "        )\n",
    "\n",
    "    def _read_data_array(self, data_dir):\n",
    "        with open(data_dir, encoding=\"utf8\") as f:\n",
    "            arr = f.readlines()\n",
    "        return arr\n",
    "\n",
    "    def _load_tokenizer(self, tokenizer_dir):\n",
    "        return Tokenizer.from_file(str(tokenizer_dir))\n",
    "\n",
    "    def _data_preprocess(self, data_dir):\n",
    "        src_txt = self._read_data_array(Path(data_dir) / \"ch.txt\")\n",
    "        trg_txt = self._read_data_array(Path(data_dir) / \"jp.txt\")\n",
    "        parallel_txt = np.array(list(zip(src_txt, trg_txt)))\n",
    "        return parallel_txt\n",
    "\n",
    "    def _data_batching_fn(self, data_batch):\n",
    "        data_batch = np.array(data_batch)  # shape=(batch_size, 2=src+trg)\n",
    "\n",
    "        src_batch = data_batch[:, 0]  # shape=(batch_size, )\n",
    "        trg_batch = data_batch[:, 1]  # shape=(batch_size, )\n",
    "        \n",
    "        # src_batch=(batch_size, longest_sentence)\n",
    "        # trg_batch=(batch_size, longest_sentence)\n",
    "        src_batch = self.src_tokenizer.encode_batch(src_batch)  \n",
    "        trg_batch = self.trg_tokenizer.encode_batch(trg_batch)\n",
    "\n",
    "        # We have to sort the batch by their non-padded lengths in descending order,\n",
    "        # because the descending order can help in `nn.utils.rnn.pack_padded_sequence()`,\n",
    "        # which it will help us ignoring the <pad> in training rnn.\n",
    "        # https://meetonfriday.com/posts/4d6a906a\n",
    "        src_batch, trg_batch = zip(\n",
    "            *sorted(\n",
    "                zip(src_batch, trg_batch),\n",
    "                key=lambda x: sum(x[0].attention_mask),\n",
    "                reverse=True,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return src_batch, trg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d7d4ac4-bfd6-486e-8b0b-faeb49fa4f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencepiece_dm_art = run.use_artifact(\"sentencepiece_dm:latest\")\n",
    "sentencepiece_dm_dir = sentencepiece_dm_art.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afa8036a-ca16-4ce3-b6b8-a9e4522d18dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm = SentencePieceDataModule(data_dir, ch_tokenizer_dir, jp_tokenizer_dir, 128)\n",
    "with open(Path(sentencepiece_dm_dir) / \"sentencepiece_dm.pkl\", \"rb\") as f:\n",
    "    dm = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce56a9fb-f0c4-4251-b732-b730903391cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.batch_size = 16\n",
    "dm.num_workers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c617a9-9850-41d4-be65-b3a29fb199d1",
   "metadata": {},
   "source": [
    "### Test DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55802c0c-afb3-4adc-8f65-d86c1f239074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dm.setup(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad0b6f57-2e4e-426d-a70c-308bfaf21921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000 32000\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "input_dim = dm.src_tokenizer.get_vocab_size()\n",
    "output_dim = dm.trg_tokenizer.get_vocab_size()\n",
    "print(input_dim, output_dim)\n",
    "\n",
    "src_pad_idx = dm.src_tokenizer.token_to_id(\"[PAD]\")\n",
    "print(src_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5284cdb6-cbdb-462b-bf59-f961677ba687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 Encoding(num_tokens=71, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "16 Encoding(num_tokens=64, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "for src, trg in dm.test_dataloader():\n",
    "    print(len(src), src[0])\n",
    "    print(len(trg), trg[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a82584-c3f8-4247-9154-1560c8c7a088",
   "metadata": {},
   "source": [
    "# Build Lightning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b14a37f-d343-4e7e-808b-84d29b3bc4b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Encoder\n",
    "\n",
    "![](../assets/bi_encoder.png)\n",
    "\n",
    "é¦–å…ˆæˆ‘å€‘å…ˆè¼¸å…¥ embeded éå¾Œçš„å­—ä¾†è¨ˆç®—æ­£å‘å’Œåå‘çš„ hidden state:\n",
    "\n",
    "$$\n",
    "h_\\overrightarrow{t} = \\overrightarrow{\\text{EncoderGRU}}(\\text{emb}(x_\\overrightarrow{t}), h_\\overrightarrow{t-1})\n",
    "\\\\\n",
    "h_\\overleftarrow{t} = \\overleftarrow{\\text{EncoderGRU}}(\\text{emb}(x_\\overleftarrow{t}), h_\\overleftarrow{t-1})\n",
    "$$\n",
    "\n",
    "å¾—åˆ°çš„ `outputs` ä»£è¡¨æ‰€æœ‰æœ€å¾Œä¸€å±¤çš„ hidden states çš„çµ„åˆï¼Œæˆ‘å€‘æœƒç”¨ outputs ä¾†è¨ˆç®— attentionï¼Œä¹Ÿå°±æ˜¯ç¿»è­¯æ™‚è¦æ³¨æ„åŸå¥çš„å“ªäº›å–®å­—:\n",
    "\n",
    "$$\n",
    "h_1 = [h_\\overrightarrow{1}; h_\\overleftarrow{1}], h_2 = [h_\\overrightarrow{2}; h_\\overleftarrow{2}], \\\\\n",
    "\\text{outputs} = H = \\left\\{h_1, h_2, \\cdots, h_T\\right\\}\n",
    "$$\n",
    "\n",
    "å¾—åˆ°çš„ `hidden` ä»£è¡¨æ¯ä¸€å±¤æœ€å¾Œä¸€å€‹æ™‚é–“é»çš„ hidden states çš„ç–ŠåŠ ï¼Œæˆ‘å€‘æœƒç”¨ hidden åšç‚º decoder åˆå§‹çš„ context vector `s0`:\n",
    "\n",
    "$$\n",
    "\\overrightarrow{z} = h_\\overrightarrow{T} \\\\\n",
    "\\overleftarrow{z} = h_\\overleftarrow{T}\n",
    "$$\n",
    "\n",
    "å› ç‚º decoder ä¸æ˜¯é›™å‘ï¼Œæ‰€ä»¥æˆ‘å€‘æŠŠ hidden ä¸Ÿé€²ä¸€å€‹ linear `g` å’Œ `tanh` è£¡ç²å¾—æ¿ƒç¸®å¾Œçš„ context vector `z`:\n",
    "\n",
    "$$\n",
    "z = \\tanh(g(\\text{cat}(\\overrightarrow{z}, \\overleftarrow{z}))) = s_0\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "| variables   | use                                             | note                                              |\n",
    "| ----------- | ----------------------------------------------- | ------------------------------------------------- |\n",
    "| src         | åˆå§‹èªè¨€è³‡æ–™                                    | `shape=[batch_size, src_len]` (batch-first shape) |\n",
    "| src_len     | batch ä¸­æ¯å€‹å¥å­çš„çœŸå¯¦é•·åº¦                      | `shape=[batch_size]`                              |\n",
    "| input_dim   | åˆå§‹èªè¨€çš„ vocab_size                           | `src_tokenizer.get_vocab_size()`                  |\n",
    "| emb_dim     | embedding_size                                  ||\n",
    "| enc_hid_dim | Encoder ä¸­ rnn çš„ hidden_size                   ||\n",
    "| dec_hid_dim | Decoder ä¸­ rnn çš„ hidden_size                   ||\n",
    "| rnn         | é›™å‘ GRU, åƒ batch-first çš„è³‡æ–™                 | `nn.GRU(emb_dim, enc_hid_dim, bidirectional = True, batch_first=True)`|\n",
    "| fc          | å°‡é›™å‘ context vector è¼¸å‡ºæˆå–®å€‹ context vector | `nn.Linear(enc_hid_dim * 2, dec_hid_dim)`|\n",
    "\n",
    "### Outputs\n",
    "\n",
    "| variables        | use                                            | note                                     |\n",
    "| ---------------- | ---------------------------------------------- | ---------------------------------------- |\n",
    "| packed_embedded  | å°‡ `[PAD]` åˆªæ‰åŒ…è£æˆ packed æ ¼å¼              | `PackedSequence`                         |\n",
    "| packed_outputs   | æ²’æœ‰ `[PAD]` çš„æœ€å¾Œä¸€å±¤ hidden_states          | `PackedSequence`                         |\n",
    "| enc_outputs      | æœ‰ `[PAD]` çš„æœ€å¾Œä¸€å±¤ hidden_states            | `shape=[batch_size, src_len, enc_hid_dim*2]` |\n",
    "| hidden           | æ‰€æœ‰ layer ç–ŠåŠ çš„ context_vector               | `shape=[layer*2, batch_size, enc_hid_dim]`   |\n",
    "| hidden[:2, :, :] | æœ€ä¸Šé¢ forward layer çš„ hidden_state           | `shape=[batch_size, enc_hid_dim]`            |\n",
    "| hidden[:1, :, :] | æœ€ä¸Šé¢ backward layer çš„ hidden_state          | `shape=[batch_size, enc_hid_dim]`            |\n",
    "| last_hidden      | é€é torch.cat çµ„åˆæœ€å¾Œä¸€å±¤ forward + backward | `shape=[batch_size, enc_hid_dim*2]`          |\n",
    "| dec_hidden       | ç¶“é tanh + fc å¾—åˆ°çš„ context_vector           | `shape=[batch_size, dec_hid_dim]`\n",
    "\n",
    "> - **Terminology Alert**ğŸ˜ª:\n",
    ">   - **outputs** å¯ä»¥æƒ³æˆæ‰€æœ‰æ™‚é–“é»çš„æœ€å¾Œä¸€å±¤çš„ hidden_states æ‰€çµ„æˆ\n",
    ">   - **hidden** å¯ä»¥æƒ³æˆæ‰€æœ‰ layer (forward + backward) åœ¨æœ€å¾Œæ™‚é–“é»çš„ hidden_states å †ç–Šè€Œæˆçš„ context_vector\n",
    "\n",
    "å•é¡Œä¸€: ä»€éº¼æ˜¯ packed_sequence?\n",
    "> - [[Pytorch]Pack the data to train variable length sequences](https://meetonfriday.com/posts/4d6a906a)\n",
    "\n",
    "å•é¡ŒäºŒ: outputs å’Œ hidden å·®åœ¨å“ª?\n",
    "> - [å­¦ä¼šåŒºåˆ† RNN çš„ output å’Œ state](https://zhuanlan.zhihu.com/p/28919765)\n",
    "> - [LSTM/GRUä¸­outputå’Œhiddençš„åŒºåˆ«//å…¶ä»–é—®é¢˜](https://blog.csdn.net/yagreenhand/article/details/84893493)\n",
    "\n",
    "<img src=\"../assets/output_vs_hidden.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbdf3cb7-3b7c-46d4-a57e-620868022575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_len):\n",
    "        # src     = [batch_size, src_len]\n",
    "        # src_len = [batch_size]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to(\"cpu\"), batch_first=True)\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "        # packed_outputs is a packed sequence containing all hidden states\n",
    "        # hidden is now from the final non-padded element in the batch\n",
    "\n",
    "        enc_outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "        # enc_outputs is now a non-packed sequence\n",
    "\n",
    "        # enc_outputs = [batch_size, src_len, enc_hid_dim*num_directions]\n",
    "        #             = [forward_n + backward_n]\n",
    "        #             = [last layer]\n",
    "\n",
    "        # hidden  = [n_layers*num_directions, batch_size, enc_hid_dim]\n",
    "        #         = [forward_1, backward_1, forward_2, backword_2, ...]\n",
    "\n",
    "        # hidden[-2, :, : ] is the last of the forwards RNN\n",
    "        # hidden[-1, :, : ] is the last of the backwards RNN\n",
    "\n",
    "        last_hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        init_dec_hidden = torch.tanh(self.fc(last_hidden))\n",
    "\n",
    "        # enc_outputs     = [batch_size, src_len, enc_hid_dim*2]  (we only have 1 layer)\n",
    "        # init_dec_hidden = [batch_size, dec_hid_dim]\n",
    "\n",
    "        return enc_outputs, init_dec_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f688f0a0-d19b-4016-8df1-bc5cb1dc9786",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Attention\n",
    "\n",
    "![](../assets/seq2seq_encoder_attention.png)\n",
    "\n",
    "å…ˆèªªçµè«–ï¼Œæ¯æ¬¡ attention layer æœƒç”¢ç”Ÿä¸€å€‹ `src_len` é•·åº¦çš„é™£åˆ—ï¼Œä»£è¡¨åœ¨é æ¸¬ä¸‹ä¸€å€‹å­— $\\hat{y}_{t+1}$ çš„æ™‚é–“é»æ™‚ï¼Œå°åŸå¥ `src` ä¸­æ¯ä¸€å€‹ token çš„å°ˆæ³¨åº¦æœ‰å¤šé«˜ã€‚\n",
    "\n",
    "ğŸ¤¯ æ¯æ¬¡éœ€è¦çµ¦ attention layer ä»€éº¼?\n",
    "\n",
    "1. decoder å‰ä¸€å€‹æ™‚é–“é»çš„ hidden state $s_{t-1}$ (i.e., ç¬¬ä¸€å€‹å°±æ˜¯ encoder çš„ `hidden` $z$ ä¹Ÿå°±æ˜¯ $s_0$)\n",
    "2. encoder çš„ `outputs` $H$\n",
    "\n",
    "è€Œ attention layer å…¶å¯¦åªæ˜¯ä¸€å€‹ linear layerï¼Œç”¨ä¾†å’Œ `tanh` ä¸€èµ·è¨ˆç®—å‡ºä¸€å€‹èƒ½é‡å€¼ $E_t$:\n",
    "\n",
    "$$\n",
    "E_t = \\tanh(\\text{attn}(s_{t-1}, H))\n",
    "$$\n",
    "\n",
    "å› ç‚º `enc_outputs` çš„é•·åº¦æ˜¯ `src_len`ï¼Œè€Œ `hidden` åªæ˜¯ä¸€å€‹ scalarï¼Œæ‰€ä»¥æˆ‘å€‘å¿…é ˆæŠŠ `hidden` æ‹‰åˆ°è·Ÿ `enc_outputs` ä¸€æ¨£é•·ã€‚ è¨ˆç®—å‡ºä¾†çš„ $E_t$ å¯ä»¥æƒ³åƒæˆ `encoder_outputs` $H$ å’Œ `previous_decoder_hidden_state` $s_{t-1}$ æœ‰å¤šåŒ¹é…ã€‚\n",
    "\n",
    "å› ç‚ºç®—å‡ºä¾†çš„èƒ½é‡å€¼ $E_t$ å½¢ç‹€æ˜¯ `[src_len, hid_dim]`ï¼Œæˆ‘å€‘å¯ä»¥æŠŠä»–å¸¶å…¥ä¸€å€‹å½¢ç‹€æ˜¯ `[hid_dim, 1]` linear layer $v$ã€‚æœ€çµ‚å¾—åˆ°ä¸€å€‹å½¢ç‹€æ˜¯ `[src_len]` çš„ `attention_sequence`:\n",
    "\n",
    "$$\n",
    "\\hat{a}_t = v(E_t)\n",
    "$$\n",
    "\n",
    "ä½ å¯ä»¥æƒ³åƒ $v$ è£¡é¢å­¸ç¿’åˆ°çš„åƒæ•¸æ˜¯ä¸€å€‹æ¬Šé‡ï¼Œå‘Šè¨´æˆ‘å€‘èƒ½é‡å€¼ $E_t$ ä½œç”¨åœ¨ `encoder_outputs` ä¸­æ¯å€‹ token çš„æ¬Šé‡æœ‰å¤šå°‘ã€‚\n",
    "\n",
    "æœ€å¾Œçš„æœ€å¾Œï¼Œ attention_sequence æœƒé€šé softmax è®“æ‰€æœ‰æ©Ÿç‡åŠ ç¸½ç‚º 1ï¼Œå…¶ä¸­æœƒæŠŠ `attention_sequence` å’Œ `mask` çµåˆï¼Œè®“å°æ‡‰åœ¨ [PAD] index çš„ hidden state éƒ½è®Šæˆ -1e10 (æœƒè®“ä»–å€‘åœ¨å¥—å…¥ softmax å¾Œè®Šæˆ 0)ã€‚\n",
    "\n",
    "$$\n",
    "a_t = \\text{softmax}(\\hat{a}_t)\n",
    "$$\n",
    "\n",
    "é€™å€‹ $a_t$ æ­£æ˜¯å‘Šè¨´æˆ‘å€‘åœ¨ decode çš„ç•¶ä¸‹ï¼Œè¦æ³¨è¦–åŸå¥çš„å“ªäº› token!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "| variable        | use                                                   | note                                          |\n",
    "| --------------- | ----------------------------------------------------- | --------------------------------------------- |\n",
    "| hidden          | encoder_hidden $s_0$ æˆ–æ˜¯ decoder_hidden $s_{t-1}$     | `shape=[batch_size, dec_hid_dim]`              |\n",
    "| encoder_outputs | encoder_outputs $H$ï¼Œä¹Ÿå°±æ˜¯ encoder æœ€å¾Œä¸€å±¤çš„ hidden_states   | `shape=[batch_size, src_len, enc_hid_dim * 2]` |\n",
    "| mask            | ç”¨ä¾†é®ç½©çš„ tensorï¼Œ1 æ˜¯çœŸå¯¦çš„ tokenï¼Œ0 æ˜¯ [PAD]            | `shape=[batch_size, src_len]`      |\n",
    "| attn            | ç”¨ä¾†åŒ¹é… `enc_outputs` å’Œ `hidden` çš„ attention layer | `linear(enc_hid*2+dec_hid, dec_hid)` |\n",
    "| v               | ç”¨ä¾†å­¸ç¿’ `attention` æ¬Šé‡çš„ linear layer              | `linear(dec_hid, 1)`                 |\n",
    "\n",
    "### Outputs\n",
    "\n",
    "| variable              | use                                                                                                       | note                                       |\n",
    "| --------------------- | --------------------------------------------------------------------------------------------------------- | ------------------------------------------ |\n",
    "| energy                | è¨ˆç®— attention sequence çš„ç¬¬ä¸€å€‹ç”¢ç‰©ï¼Œå°‡ hidden+encoder_outputs çµ„åˆå¾Œï¼Œé€é attention_layer å’Œ tanh ç®—å‡º | `shape=[batch_size, src_len, dec_hid_dim]` |\n",
    "| attention             | å°‡èƒ½é‡å€¼ `energy` ä¸Ÿå…¥ `v` ä¸­å­¸ç¿’æ¬Šé‡å¾Œç”¢ç”Ÿçš„ attention sequenceï¼Œä½†é‚„æ²’è™•ç† padding                      | `shape=[batch_size, src_len]`              |\n",
    "| attention.masked_fill | æŠŠ attention sequence ç•¶ä¸­ï¼Œindex æ˜¯ [PAD] çš„åœ°æ–¹æ”¹æˆ -1e10ï¼Œè®“ä»–å€‘é€šé softmax éƒ½æœƒè®Šæˆ 0    | `tensor.masked_fill(mask, dim=n)` |                                                                                                         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "340b0be0-315e-4edf-82b9-6aec15abdc79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "\n",
    "        # hidden = [batch_size, dec_hid_dim]\n",
    "        # encoder_outputs = [batch_size, src_len, enc_hid_dim * 2]\n",
    "\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        # hidden = [batch_size, 1, dec_hid_dim]        (unsqueeze 1)\n",
    "        #        = [batch_size, src_len, dec_hid_dim]  (repeat)\n",
    "        \n",
    "        stacked_hidden = torch.cat((hidden, encoder_outputs), dim=2)\n",
    "        # stacked_hidden = [batch_size, src_len, dec_hid_dim + enc_hid_dim * 2]\n",
    "\n",
    "        energy = torch.tanh(self.attn(stacked_hidden))\n",
    "        # energy = [batch_size, src_len, dec_hid_dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        # attention = [batch_size, src_len, 1]   (v)\n",
    "        #           = [batch_size, src_len]      (squeeze)\n",
    "\n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a00635-74e1-4304-a5e0-195e00c6bb33",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Decoder\n",
    "\n",
    "![](../assets/seq2seq_decoder_attention.png)\n",
    "\n",
    "Attention æ©Ÿåˆ¶æœƒç”¨å‰ä¸€å€‹æ™‚é–“é» $t-1$ çš„ `hidden` $s_{t-1}$ å’Œä»£è¡¨æ•´å€‹åŸå¥çš„ `encoder_outputs` $H$ï¼Œè¨ˆç®—å‡ºç¾åœ¨æ™‚é–“é» $t$ çš„ attention vector $a_t$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E_t &= \\tanh(\\text{attn}(s_{t-1}, H)) \\\\\n",
    "\\hat{a}_t &= vE_t \\\\\n",
    "a_t &= \\text{softmax}(\\hat{a}_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "æ¥è‘—ï¼Œæˆ‘å€‘å†ä½¿ç”¨ $a_t$ å° $H$ é€²è¡Œ `matrix-matrix product`ï¼Œæ‰¾å‡ºçœŸæ­£èƒ½è¡¨é”åŸå¥ä¸­ï¼Œå°æ¯å€‹ token å°ˆæ³¨åŠ›çš„ `weighted_sum` $w_t$ã€‚\n",
    "\n",
    "$$\n",
    "w_t = a_tH\n",
    "$$\n",
    "\n",
    "æ¥è‘—å°±å¯ä»¥é€šé `decoderGRU` è¨ˆç®—ç¾åœ¨æ™‚é–“é» $t$ çš„ `hidden` $s_t$:\n",
    "\n",
    "$$\n",
    "s_t = \\text{DecoderGRU}(d(y_t), w_t, s_{t-1})\n",
    "$$\n",
    "\n",
    "ææ–™æœ‰:\n",
    "\n",
    "1. embedded token $d(y_t)$ (ä¾‹åœ–ä¸­ decoder çš„ `<sos>` ç¶“é embedding çš„çµæœ)\n",
    "2. ä¸Šé¢ç®—å‡ºä¾†çš„ weighted source vector $w_t$\n",
    "3. å‰ä¸€å€‹æ™‚é–“é»çš„ decoder çš„ `hidden` $s_{t-1}$\n",
    "\n",
    "é æ¸¬ä¸‹ä¸€å€‹ token $\\hat{y}_{t+1}$ å°±å¾ˆç°¡å–®äº†ï¼Œåªè¦æŠŠæ±è¥¿éƒ½å‚™é½Šï¼Œæ”¾é€² linear layer `fc_out` å°±å¥½:\n",
    "\n",
    "$$\n",
    "\\hat{y}_{t+1} = f(d(y_t), w_t, s_t)\n",
    "$$\n",
    "\n",
    "### Inputs\n",
    "\n",
    "| variable        | use                                                | note                                                        |\n",
    "| --------------- | -------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| inp           | åœ¨ç¾åœ¨æ™‚é–“é» $t$ æ™‚è¼¸å…¥åˆ° decoder çš„ token         | `shape=[batch_size]`                                        |\n",
    "| hidden          | å‰ä¸€å€‹æ™‚é–“é» $t-1$ çš„ hidden state                 | `shape=[batch_size, dec_hid_dim]`                           |\n",
    "| encoder_outputs | Encoder æœ€å¾Œä¸€å±¤çš„ hidden_states $H$               | `shape=[batch_size, src_len, enc_hid_dim*2]`                |\n",
    "| mask            | çµ¦ attention ç”¨ä¾†ç„¡è¦– `[PAD]` çš„ 0/1s              | `shape=[batch_size, src_len]`                               |\n",
    "| output_dim      | ç›®æ¨™èªè¨€çš„ `vocab_size`ï¼Œç”¨ä¾†ç•¶ embedding è¼¸å‡ºå¤§å° | `trg_tokenizer.get_vocab_size()`                            |\n",
    "| rnn             | å–®å±¤ä¸”å–®å‘çš„ GRUï¼Œåƒ batch_first çš„è³‡æ–™            | `GRU(enc_hid_dim*2+emb_dim, dec_hid_dim, batch_first=True)` |\n",
    "| fc_out          | é æ¸¬ä¸‹ä¸€å€‹ token çš„ linear layer                   | `Linear(enc_hid_dim*2+dec_hid_dim+emb_dim, output_dim)`     |\n",
    "\n",
    "### Outputs\n",
    "\n",
    "| variable   | use                                                  | note                                          |\n",
    "| ---------- | ---------------------------------------------------- | --------------------------------------------- |\n",
    "| a          | attention vector                                     | `shape=[batch_size, src_len]`                 |\n",
    "| embedded   | input token ç¶“é embedding å¾—åˆ°çš„çµæœ                | `shape=[batch_size, emb_dim]`                 |\n",
    "| weighted   | attention å’Œ encoder_outputs ä¹˜ç©å¾—åˆ°çš„ weighted sum | `shape=[batch_size, src_len]`                 |\n",
    "| rnn_input  | embedded å’Œ weighted å †ç–Š                            | `shape=[batch_size, enc_hid_dim*2 + emb_dim]` |\n",
    "| output     | DecoderGRU çš„ hidden state $s_t$                     | `shape=[batch_size, dec_hid_dim]`             |\n",
    "| hidden     | DecoderGRU çš„ hidden state $s_t$                     | `shape=[batch_size, dec_hid_dim]`             |\n",
    "| prediction | é æ¸¬ä¸‹ä¸€å€‹ token æ˜¯å­—å…¸ä¸­å“ªä¸€å€‹ token çš„æ©Ÿç‡åˆ†å¸ƒ     | `shape=[batch_size, output_dim]`              |\n",
    "\n",
    "> 1. forward ä¸­å¾ˆå¤šå‘é‡éƒ½æ“´å……äº†ä¸€å€‹ç¶­åº¦ï¼Œé‚£æ˜¯ä»£è¡¨ seq_len=1\n",
    "> 2. å› ç‚º decoderGRU åªæœ‰å–®å±¤ã€å–®æ™‚é–“é»ï¼Œæ‰€ä»¥ output å’Œ hidden æ˜¯ä¸€æ¨£çš„æ±è¥¿ï¼éƒ½æ˜¯ $s_t$\n",
    "> 3. `torch.bmm` æ˜¯ç°¡å–®çš„çŸ©é™£ç›¸ä¹˜\n",
    ">     1. ä¸€å®šè¦ 3 ç¶­çŸ©é™£\n",
    ">     2. å…¬å¼æ˜¯ $b\\times n\\times m @ b\\times m\\times p = b\\times n\\times p$\n",
    ">     3. `bmm((10, 3, 4), (10, 4, 5)) = (10, 3, 5)`\n",
    ">     4. [documentation](https://pytorch.org/docs/stable/generated/torch.bmm.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "784d865d-b849-42bf-895b-c6553db52b50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inp, hidden, encoder_outputs, mask):\n",
    "        # encoder_outputs = [batch_size, src_len, enc_hid_dim*2]\n",
    "        # hidden = [batch_size, dec_hid_dim]\n",
    "\n",
    "        inp = inp.unsqueeze(1)\n",
    "        # inp = [batch_size]\n",
    "        #     = [batch_size, 1]  (unsqueeze 1)\n",
    "\n",
    "        # embedded = [batch_size, 1, emb_dim]\n",
    "        embedded = self.dropout(self.embedding(inp))\n",
    "        \n",
    "        # a = [batch_size, src_len]\n",
    "        #   = [batch_size, 1, src_len]  (unsqueeze 1)\n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        # weighted = [batch_size, 1, enc_hid_dim*2]\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "\n",
    "        # rnn_input = [batch_size, 1, emb_dim + enc_hid_dim*2]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        \n",
    "        # hidden = [1, batch_size, dec_hid_dim]  (unsqueeze 0)\n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        \n",
    "        # output = [batch_size, 1, dec_hid_dim]\n",
    "        # hidden = [1, batch_size, dec_hid_dim]\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        # embedded = [batch_size, emb_dim]        (squeeze 0)\n",
    "        # output   = [batch_size, dec_hid_dim]    (squeeze 0)\n",
    "        # weighted = [batch_size, enc_hid_dim*2]  (squeeze 0)\n",
    "        # hidden = [batch_size, dec_hid_dim]      (squeeze 0)\n",
    "        embedded = embedded.squeeze(1)\n",
    "        output = output.squeeze(1)\n",
    "        weighted = weighted.squeeze(1)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        \n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        predict_input = torch.cat((output, weighted, embedded), dim=1)\n",
    "\n",
    "        # prediction = [batch_size, output_dim]\n",
    "        prediction = self.fc_out(predict_input)\n",
    "\n",
    "        # a = [batch_size, src_len]  (squeeze 1)\n",
    "        a = a.squeeze(1)\n",
    "\n",
    "        return prediction, hidden, a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0b337c-0f6f-48e8-9528-24279c6d657f",
   "metadata": {},
   "source": [
    "## Full Seq2Seq Model\n",
    "\n",
    "æˆ‘ç”¨ `pl.LightningModule` ä¾†å°è£æ‰€æœ‰ seq2seq æ¨¡å‹çš„ training å’Œ validation (ä½¿ç”¨è‡ªå·±çš„ `_forward()` å‡½å¼)ã€ä»¥åŠ test step (ä½¿ç”¨å…§å»ºçš„ `forward()` å‡½å¼)ã€‚è¼¸å…¥çš„ `config` ç‚ºç¶²è·¯ä¸­æ‰€æœ‰å¯ä»¥è¢«èª¿æ•´çš„è¶…åƒæ•¸ï¼Œå¯ä»¥ç”¨æ–¼åŸ·è¡Œ `wandb sweep` (hyperparameter tuning)ã€‚\n",
    "\n",
    "### Training\n",
    "\n",
    "åœ¨ seq2seq ä¸­ï¼Œè¨“ç·´æ™‚ (`_forward()`) é¦–å…ˆå¾ encoder ç²å¾—å…©ç¨® final_hidden_states (åˆ†åˆ¥æ˜¯ outputs å’Œ hidden):\n",
    "\n",
    "1. outputs: ç”±æ¯å€‹æ™‚é–“é»çš„ final_linear è¼¸å‡ºçš„ hidden_states ç–ŠåŠ è€Œæˆï¼Œä½œç‚º attention ç”¨é€”\n",
    "2. hidden: ç”±æœ€å¾Œä¸€å€‹æ™‚é–“é»çš„æ‰€æœ‰ hidden_states çµ„åˆè€Œæˆï¼Œä½œç‚ºåˆå§‹çš„ decoder_hidden_states\n",
    "\n",
    "å†ä¾†å°±æ˜¯ decoder è¨“ç·´çš„éƒ¨åˆ†:\n",
    "\n",
    "- `preds` ç”¨ä¾†å„²å­˜æ‰€æœ‰é æ¸¬ $\\hat{y}$ çš„çµæœ\n",
    "- å°‡æ‰€æœ‰ (ä¸€å€‹ batch) è¦æ”¾å…¥ decoder çš„ input_tokens éƒ½è¨­ç‚º `[BOS]`\n",
    "- åœ¨ loop è£¡é¢é€²è¡Œ decode:\n",
    "    - å¾€ decoder ä¸Ÿå…¥ input_token $y_t$ å’Œå‰ä¸€å€‹ hidden_state $s_{t-1}$ åŠ encoder_outputs $H$\n",
    "    - ç²å¾—é æ¸¬å€¼ $\\hat{y}_{t+1}$ å’Œæ–°çš„ hidden_state $s_t$\n",
    "    - æ©Ÿç‡æ€§ä½¿ç”¨ `teacher_force`:\n",
    "        - ä½¿ç”¨: ä¸‹ä¸€æ¬¡çš„ input_token æ˜¯ ground_truth\n",
    "        - ä¸ä½¿ç”¨: ä¸‹ä¸€æ¬¡çš„ input_token å°±æ˜¯æœ¬æ¬¡é æ¸¬ $\\hat{y}_{t+1}$\n",
    "\n",
    "decode çš„é †åºæ˜¯å¾ 1 é–‹å§‹ï¼Œé€™æ˜¯ç‚ºäº†è®“ `preds` èƒ½å¤ è·Ÿ target å°ç¨±ï¼Œç•¶æˆ‘å€‘è¦è¨ˆç®— loss æ™‚ï¼Œå†æŠŠ target å’Œ `preds` çš„ç¬¬ä¸€å€‹ç æ‰å°±å¥½:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{trg} &= \\begin{bmatrix} &\\text{[BOS]}, &y_1, &y_2, &y_3, &\\text{[EOS]} \\end{bmatrix} \\\\\n",
    "\\text{preds} &= \\begin{bmatrix} &&&0, &\\hat{y}_1, &\\hat{y}_2, &\\hat{y}_3, &\\text{[EOS]} \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "### Inference\n",
    "\n",
    "åœ¨åš inference (`forward()`) çš„æ™‚å€™ï¼Œé™¤äº†ä¸æœƒç”¨ `teacher_force` å¤–ï¼Œæˆ‘å€‘çš„ decode loop æœƒå¾ 1 è·‘åˆ°è‡ªå®šç¾©çš„ `max_len`ï¼Œè®“ decode åŸ·è¡Œåˆ° `max_len` çµæŸç‚ºæ­¢ã€‚æˆ‘æœƒåœ¨å…¨éƒ¨ batch éƒ½é æ¸¬å®Œæˆå¾Œï¼Œå†ä¾†åˆ‡æ‰ä»»ä½•å¥å­å‡ºç¾ `[EOS]` ä¹‹å¾Œçš„ tokensã€‚\n",
    "\n",
    "``` python\n",
    "eos_pos = dict((preds == self.trg_tokenizer.token_to_id(\"[EOS]\")).nonzero().tolist())\n",
    "\n",
    "real_sentence = sentence[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None]\n",
    "real_attention = attention[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None, :src_len]\n",
    "```\n",
    "\n",
    "å¦å¤–åœ¨ inference æœƒåŒæ™‚è¨˜éŒ„ attention_matrix ä½œç‚º case study ç”¨é€”ã€‚åŸ·è¡Œå®Œæ‰€æœ‰çš„é æ¸¬å¾Œï¼Œæœƒå°æ¯ä¸€å€‹è¦é æ¸¬çš„å¥å­å›å‚³å››å€‹ç‰©ä»¶ (`test_outputs`):\n",
    "\n",
    "|variable|shape|desc|\n",
    "|-|-|-|\n",
    "|pred_sentence | [trg_len] | é æ¸¬çš„å¥å­ tokens |\n",
    "|attn_matrix   | [trg_len, src_len] | pred_sentence å’Œ src_sentence çš„å°ˆæ³¨åŠ›çŸ©é™£ |\n",
    "|src_sentence  | [src_len] | åŸå¥ tokens |\n",
    "|trg_sentence  | [trg_len] | ç›®æ¨™å¥ tokens |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6244b75b-8414-4aaa-abf6-e76fe742f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(pl.LightningModule):\n",
    "    def __init__(self, input_dim, output_dim, src_tokenizer, trg_tokenizer, config):\n",
    "        super().__init__()\n",
    "        self.src_pad_idx = src_tokenizer.token_to_id(\"[PAD]\")\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            input_dim,\n",
    "            config[\"enc_emb_dim\"],\n",
    "            config[\"enc_hid_dim\"],\n",
    "            config[\"dec_hid_dim\"],\n",
    "            config[\"enc_dropout\"],\n",
    "        )\n",
    "\n",
    "        attn = Attention(config[\"enc_hid_dim\"], config[\"dec_hid_dim\"])\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            output_dim,\n",
    "            config[\"dec_emb_dim\"],\n",
    "            config[\"enc_hid_dim\"],\n",
    "            config[\"dec_hid_dim\"],\n",
    "            config[\"dec_dropout\"],\n",
    "            attn,\n",
    "        )\n",
    "\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.apply(self.init_weights)\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "            else:\n",
    "                nn.init.constant_(param.data, 0)\n",
    "    \n",
    "    \n",
    "    # Training\n",
    "    # Use only when training and validation\n",
    "    def _forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g., if teacher_forcing_ratio is 0.5 we use teacher forcing 50% of the time\n",
    "\n",
    "        # src = list of Encoding([ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
    "        # trg = list of Encoding([ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
    "\n",
    "        # src_batch = [batch_size, src_len]\n",
    "        # src_mask  = [batch_size, src_len]\n",
    "        # src_len   = [batch_size]\n",
    "        src_batch = torch.tensor([e.ids for e in src], device=self.device)\n",
    "        src_mask = torch.tensor([e.attention_mask for e in src], device=self.device)\n",
    "        src_len = torch.sum(src_mask, axis=1)\n",
    "\n",
    "        # trg_batch = [batch_size, trg_len]\n",
    "        trg_batch = torch.tensor([e.ids for e in trg], device=self.device)\n",
    "\n",
    "        batch_size = src_batch.shape[0]\n",
    "        trg_len = trg_batch.shape[1]\n",
    "        trg_vocab_size = self.output_dim\n",
    "\n",
    "        # create a tensor for storing all decoder outputs\n",
    "        preds = torch.zeros(batch_size, trg_len, trg_vocab_size, device=self.device)\n",
    "\n",
    "        # encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        # hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src_batch, src_len)\n",
    "        \n",
    "        # first input to the decoder = [BOS] tokens\n",
    "        # inp = [batch_size]\n",
    "        inp = trg_batch[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            # pred   = [batch_size, output_dim]\n",
    "            # hidden = [batch_size, dec_hid_dim]\n",
    "            pred, hidden, _ = self.decoder(inp, hidden, encoder_outputs, src_mask)\n",
    "\n",
    "            # store predictions in a tensor holding predictions for each token\n",
    "            preds[:, t, :] = pred\n",
    "            \n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # top1 = [batch_size]\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = pred.argmax(1)\n",
    "\n",
    "            # inp = [batch_size]\n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            inp = trg_batch[:, t] if teacher_force else top1\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    \n",
    "    # Inference\n",
    "    # * Let you use the pl model as a pytorch model.\n",
    "    # * \n",
    "    # * pl_model.eval()\n",
    "    # * pl_model(X)\n",
    "    # *\n",
    "    def forward(self, src, max_len=100):\n",
    "        src_batch = torch.tensor([e.ids for e in src], device=self.device)\n",
    "        src_mask = torch.tensor([e.attention_mask for e in src], device=self.device)\n",
    "        src_len = torch.sum(src_mask, axis=1)  # actual src_len without [PAD]\n",
    "        \n",
    "        batch_size = src_batch.shape[0]\n",
    "        src_size = src_batch.shape[1]  # src_len with [PAD]\n",
    "        trg_len = max_len\n",
    "        trg_vocab_size = self.output_dim\n",
    "        \n",
    "        preds = torch.zeros(batch_size, trg_len, trg_vocab_size, device=self.device)\n",
    "        encoder_outputs, hidden = self.encoder(src_batch, src_len)\n",
    "        \n",
    "        # create a tensor for storing all attention matrices\n",
    "        attns = torch.zeros(batch_size, trg_len, src_size, device=self.device)\n",
    "        \n",
    "        # first input to the decoder = [BOS] tokens\n",
    "        # inp = [batch_size]\n",
    "        inp = torch.tensor([self.trg_tokenizer.token_to_id(\"[BOS]\")], device=self.device).repeat(batch_size)\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            # attn = [batch_size, src_len]\n",
    "            pred, hidden, attn = self.decoder(inp, hidden, encoder_outputs, src_mask)\n",
    "            \n",
    "            preds[:, t, :] = pred\n",
    "            top1 = pred.argmax(1)\n",
    "            inp = top1\n",
    "            \n",
    "            # store attention sequences in a tensor holding attention value for each token\n",
    "            attns[:, t, :] = attn\n",
    "            \n",
    "        return preds, attns, src_len\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # both are lists of encodings\n",
    "        src, trg = batch\n",
    "        \n",
    "        # y    = [batch_size, trg_len]\n",
    "        # pred = [batch_size, trg_len, output_dim]\n",
    "        y = torch.tensor([e.ids for e in trg], device=self.device)\n",
    "        preds = self._forward(src, trg)\n",
    "        output_dim = preds.shape[-1]\n",
    "        \n",
    "        # y    = [batch_size * (trg_len-1)]\n",
    "        # pred = [batch_size * (trg_len-1), output_dim]\n",
    "        y = y[:, 1:].reshape(-1)\n",
    "        preds = preds[:, 1:, :].reshape(-1, output_dim)\n",
    "        \n",
    "        loss = F.cross_entropy(preds, y, ignore_index=src_pad_idx)\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        perplexity = torch.exp(loss)\n",
    "        self.log(\"train_ppl\", perplexity)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        src, trg = batch\n",
    "        y = torch.tensor([e.ids for e in trg], device=self.device)\n",
    "        preds = self._forward(src, trg)\n",
    "        \n",
    "        output_dim = preds.shape[-1]\n",
    "        y = y[:, 1:].reshape(-1)\n",
    "        preds = preds[:, 1:, :].reshape(-1, output_dim)\n",
    "        \n",
    "        loss = F.cross_entropy(preds, y, ignore_index=src_pad_idx)\n",
    "        self.log(\"valid_loss\", loss)\n",
    "        \n",
    "        perplexity = torch.exp(loss)\n",
    "        self.log(\"valid_ppl\", perplexity)\n",
    "        \n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        src, trg = batch\n",
    "        preds, attn_matrix, real_src_len = self(src)\n",
    "        \n",
    "        # attn_matrix = [batch_size, trg_len, src_len]\n",
    "        # preds       = [batch_size, trg_len, output_dim]\n",
    "        #             = [batch_size, trg_len]             (argmax 2)\n",
    "        preds = preds.argmax(2)\n",
    "        \n",
    "        # convert `preds` tensor to list of real sentences (tokens)\n",
    "        # meaning to cut the sentence by [EOS] and remove the [PAD] tokens\n",
    "        \n",
    "        # eos_pos = dict(sentence_idx: first_pad_position)\n",
    "        #\n",
    "        # e.g., {0: 32, 2: 55} \n",
    "        # Meaning that we have 32 tokens (include [EOS]) in the first predicted sentence\n",
    "        # and `max_len` tokens (no [EOS]) in the second predicted setence\n",
    "        # and 55 tokens (include [EOS]) in the third predicted sentence\n",
    "        eos_pos = dict((preds == self.trg_tokenizer.token_to_id(\"[EOS]\")).nonzero().tolist())\n",
    "        \n",
    "        pred_sentences, attn_matrices = [], []\n",
    "        for idx, (sentence, attention, src_len) in enumerate(zip(preds, attn_matrix, real_src_len)):\n",
    "            \n",
    "            # sentence  = [trg_len_with_pad]\n",
    "            #           = [real_trg_len]\n",
    "            pred_sentences.append(sentence[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None])\n",
    "            \n",
    "            # attention = [trg_len_with_pad, src_len_with_pad]\n",
    "            #           = [real_trg_len, real_src_len]\n",
    "            attn_matrices.append(attention[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None, :src_len])\n",
    "        \n",
    "        # source sentences for displaying attention matrix \n",
    "        src = [[token for token in e.tokens if token != \"[PAD]\"] for e in src]\n",
    "        \n",
    "        # target sentences for calculating BLEU scores\n",
    "        trg = [[token for token in e.tokens if token != \"[PAD]\"] for e in trg]\n",
    "        \n",
    "        return pred_sentences, attn_matrices, src, trg\n",
    "        \n",
    "    \n",
    "    def test_epoch_end(self, test_outputs):\n",
    "        outputs = []\n",
    "        for (pred_sent_list, attn_list, src_list, trg_list) in test_outputs:\n",
    "            for pred_sent, attn, src, trg in list(zip(pred_sent_list, attn_list, src_list, trg_list)):\n",
    "                pred_sent = list(map(self.trg_tokenizer.id_to_token, pred_sent))\n",
    "                outputs.append((pred_sent, attn, src, trg))\n",
    "        \n",
    "        # outputs = list of predictions of testsets, each has a tuple of (pred_sentence, attn_matrix, src_sentence, trg_sentence)\n",
    "        # pred_sentence = [trg_len]\n",
    "        # attn_matrix   = [trg_len, src_len]\n",
    "        # src_sentence  = [src_len]\n",
    "        # trg_sentence  = [trg_len]\n",
    "        self.test_outputs = outputs\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    \n",
    "    \n",
    "    def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27ba8f6d-0a19-41fc-9b84-08a66d8e5d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = pl.loggers.WandbLogger()\n",
    "\n",
    "config = {\n",
    "    \"enc_emb_dim\": 128,\n",
    "    \"dec_emb_dim\": 128,\n",
    "    \"enc_hid_dim\": 128,\n",
    "    \"dec_hid_dim\": 128,\n",
    "    \"enc_dropout\": 0.3,\n",
    "    \"dec_dropout\": 0.3,\n",
    "    \"lr\": 1e-3,\n",
    "}\n",
    "\n",
    "model = Seq2SeqModel(\n",
    "    input_dim,\n",
    "    output_dim,\n",
    "    dm.src_tokenizer,\n",
    "    dm.trg_tokenizer,\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "582203d1-ea7e-4700-a056-d79367252b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 25,085,824 trainable parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2SeqModel(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(32000, 128)\n",
       "    (rnn): GRU(128, 128, batch_first=True, bidirectional=True)\n",
       "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (v): Linear(in_features=128, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(32000, 128)\n",
       "    (rnn): GRU(384, 128, batch_first=True)\n",
       "    (fc_out): Linear(in_features=512, out_features=32000, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f511c9-2fdd-4513-870b-3cb135561fcb",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f8da6e7-48f4-4864-90f5-2826fc1ac8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = Path(\"checkpoints\")\n",
    "\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(dirpath=ckpt_dir,  # path for saving checkpoints\n",
    "                                          filename=\"best_ckpt\",\n",
    "                                          monitor=\"valid_loss\",\n",
    "                                          mode=\"min\",\n",
    "                                          save_top_k=1,\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "863b5798-7b71-4d6e-a413-9ef8a1d92bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    gpus=1,\n",
    "    max_epochs=10,\n",
    "    fast_dev_run=False,\n",
    "    gradient_clip_val=1,\n",
    "    resume_from_checkpoint=ckpt_dir / \"best_ckpt\",\n",
    "    callbacks=[checkpoint],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddce994a-6222-4edf-8f2c-1d452fdd6aa7",
   "metadata": {},
   "source": [
    "## Save Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3e08911-f53f-459c-818e-48755f50414f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f854c515c2ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_setup_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_before_accelerator_backend_setup\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# note: this sets up self.lightning_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/gpu.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, trainer, model)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_nvidia_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, trainer, model)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mto\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \"\"\"\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_training_type_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_precision_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mconnect_training_type_plugin\u001b[0;34m(self, plugin, model)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \"\"\"\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mplugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect_precision_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPrecisionPlugin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/single_device.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/single_device.py\u001b[0m in \u001b[0;36mmodel_to_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/core/decorators.py\u001b[0m in \u001b[0;36minner_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mpre_layer_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_post_move_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mpost_layer_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/device_dtype_mixin.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__update_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    669\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    670\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d1269-f95d-4682-999d-2f857328999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(model.best_model_path)\n",
    "run._config = config\n",
    "\n",
    "model_artifact = wandb.Artifact(\n",
    "            \"rnn_attention\", type=\"model\",\n",
    "            description=\"Seq2Seq Model with RNN attention implemented\",\n",
    "            metadata=dict(config))\n",
    "\n",
    "model_artifact.add_file(model.best_model_path)\n",
    "wandb.save(model.best_model_path)\n",
    "\n",
    "run.log_artifact(model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b8febf-1377-44e6-a39e-e90d69366c9e",
   "metadata": {},
   "source": [
    "# Testing (BLEU Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9922a0f-3d69-43b0-bb5f-59bec045bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_from_checkpoint(checkpoint.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844e8b93-fd94-41e0-9104-e1becb175b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b603898a-8157-44ac-996a-2e94f400dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_corpus_bleu(preds: List[str], refs: List[List[str]], n_gram=4):\n",
    "    # arg example:\n",
    "    # preds: [\"æœºå™¨äººè¡Œä¸šåœ¨ç¯å¢ƒé—®é¢˜ä¸Šçš„æªæ–½\", \"æ¾ä¸‹ç”Ÿäº§ç§‘æŠ€å…¬å¸ä¹Ÿä»¥ç¯å¢ƒå…ˆè¿›ä¼ä¸šä¸ºç›®æ ‡\"]\n",
    "    # refs: [[\"æœºå™¨äººåœ¨ç¯å¢ƒä¸Šçš„æ”¹å˜\", \"å°æ–¼æœºå™¨äººåœ¨ç¯å¢ƒä¸Šçš„æªæ–½\"],  [\"æ¾ä¸‹ç§‘æŠ€å…¬å¸çš„é¦–è¦ç›®æ ‡æ˜¯è§£å†³ç¯å¢ƒé—®é¢˜\"]]\n",
    "    preds = list(map(list, preds))\n",
    "    refs = [[list(sen) for sen in ref] for ref in refs]\n",
    "    return torchmetrics.functional.nlp.bleu_score(preds, refs, n_gram=n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be6ba1b-dccc-4fe4-92ff-ae5f6e8e901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.test_outputs[0][0]\n",
    "pred = [dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, pred)))]\n",
    "ref = model.test_outputs[0][3]\n",
    "ref = [[dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, ref)))]]\n",
    "calculate_bleu(pred, ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a0b7ac-d318-4e84-85d9-f0272b17eaa3",
   "metadata": {},
   "source": [
    "# Case Study and Attention Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad74f903-8b5f-47e1-b538-c0e7383ab03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_study(pred_token, src_token, trg_token, attn_matrix):\n",
    "    print(dm.src_tokenizer.decode(list(map(dm.src_tokenizer.token_to_id, src_token))))\n",
    "    print(dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, trg_token))))\n",
    "    print(dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, pred_token))))\n",
    "    \n",
    "    plt.rcParams['font.sans-serif'] = ['Noto Sans CJK TC']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    \n",
    "    sns.heatmap(attn_matrix, xticklabels=src_token, yticklabels=pred_token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb386ff-5c59-4784-981f-3b6dc2bfafc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_study(model.test_outputs[0][0],\n",
    "           model.test_outputs[0][2],\n",
    "           model.test_outputs[0][3],\n",
    "           model.test_outputs[0][1].cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
