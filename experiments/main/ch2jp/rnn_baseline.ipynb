{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import built-in Python libs\n",
    "import pickle\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# Import data science libs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import deep learning libs\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import weights & bias\n",
    "import wandb\n",
    "\n",
    "# Import data preprocessing libs\n",
    "from tokenizers import Tokenizer, decoders, pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import NFKC\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Datasets, Tokenizers, DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\r\n",
    "    \"enc_emb_dim\": 300,\r\n",
    "    \"dec_emb_dim\": 300,\r\n",
    "    \"enc_hid_dim\": 512,\r\n",
    "    \"dec_hid_dim\": 512,\r\n",
    "    \"enc_dropout\": 0.3,\r\n",
    "    \"dec_dropout\": 0.3,\r\n",
    "    \"lr\": 1e-3,\r\n",
    "    \"batch_size\": 64,\r\n",
    "    \"num_workers\": 12,\r\n",
    "    \"precision\": 16,\r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwindsuzu\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": "\n                Tracking run with wandb version 0.10.28<br/>\n                Syncing run <strong style=\"color:#cdcd00\">deft-planet-152</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation</a><br/>\n                Run page: <a href=\"https://wandb.ai/windsuzu/phonetic-translation/runs/24f33all\" target=\"_blank\">https://wandb.ai/windsuzu/phonetic-translation/runs/24f33all</a><br/>\n                Run data is saved locally in <code>/home/windsuzu/phonetics-in-chinese-japanese-machine-translation/experiments/main/wandb/run-20210503_105039-24f33all</code><br/><br/>\n            ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=\"phonetic-translation\",\n",
    "    entity=\"windsuzu\",\n",
    "    group=\"experiments\",\n",
    "    job_type=\"baseline_rnn\",\n",
    "    config=config,\n",
    "    reinit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact filtered_train:latest, 99.47MB. 2 files... Done. 0:0:0\n"
     ]
    }
   ],
   "source": [
    "train_data_art = run.use_artifact(\"filtered_train:latest\")\r\n",
    "train_data_dir = train_data_art.download()\r\n",
    "\r\n",
    "dev_data_art = run.use_artifact(\"dev:latest\")\r\n",
    "dev_data_dir = dev_data_art.download()\r\n",
    "\r\n",
    "test_data_art = run.use_artifact(\"test:latest\")\r\n",
    "test_data_dir = test_data_art.download()\r\n",
    "\r\n",
    "data_dir = {\r\n",
    "    \"train\": train_data_dir,\r\n",
    "    \"dev\": dev_data_dir,\r\n",
    "    \"test\": test_data_dir,\r\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencepiece_tokenizer_art = run.use_artifact(\"sentencepiece:latest\")\n",
    "sentencepiece_tokenizer_dir = sentencepiece_tokenizer_art.download()\n",
    "ch_tokenizer_dir = Path(sentencepiece_tokenizer_dir) / \"ch_tokenizer.json\"\n",
    "jp_tokenizer_dir = Path(sentencepiece_tokenizer_dir) / \"jp_tokenizer.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentencePieceDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        src_tokenizer_dir,\n",
    "        trg_tokenizer_dir,\n",
    "        batch_size=128,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.src_tokenizer_dir = src_tokenizer_dir\n",
    "        self.trg_tokenizer_dir = trg_tokenizer_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.src_tokenizer = self._load_tokenizer(self.src_tokenizer_dir)\n",
    "        self.trg_tokenizer = self._load_tokenizer(self.trg_tokenizer_dir)\n",
    "\n",
    "        if stage == \"fit\":\n",
    "            self.train_set = self._data_preprocess(self.data_dir[\"train\"])\n",
    "            self.val_set = self._data_preprocess(self.data_dir[\"dev\"])\n",
    "\n",
    "        if stage == \"test\":\n",
    "            self.test_set = self._data_preprocess(self.data_dir[\"test\"])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_set,\n",
    "            self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self._data_batching_fn,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_set,\n",
    "            self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self._data_batching_fn,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_set,\n",
    "            self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self._data_batching_fn,\n",
    "        )\n",
    "\n",
    "    def _read_data_array(self, data_dir):\n",
    "        with open(data_dir, encoding=\"utf8\") as f:\n",
    "            arr = f.readlines()\n",
    "        return arr\n",
    "\n",
    "    def _load_tokenizer(self, tokenizer_dir):\n",
    "        return Tokenizer.from_file(str(tokenizer_dir))\n",
    "\n",
    "    def _data_preprocess(self, data_dir):\n",
    "        src_txt = self._read_data_array(Path(data_dir) / \"ch.txt\")\n",
    "        trg_txt = self._read_data_array(Path(data_dir) / \"jp.txt\")\n",
    "        parallel_txt = np.array(list(zip(src_txt, trg_txt)))\n",
    "        return parallel_txt\n",
    "\n",
    "    def _data_batching_fn(self, data_batch):\n",
    "        data_batch = np.array(data_batch)  # shape=(batch_size, 2=src+trg)\n",
    "\n",
    "        src_batch = data_batch[:, 0]  # shape=(batch_size, )\n",
    "        trg_batch = data_batch[:, 1]  # shape=(batch_size, )\n",
    "        \n",
    "        # src_batch=(batch_size, longest_sentence)\n",
    "        # trg_batch=(batch_size, longest_sentence)\n",
    "        src_batch = self.src_tokenizer.encode_batch(src_batch)  \n",
    "        trg_batch = self.trg_tokenizer.encode_batch(trg_batch)\n",
    "\n",
    "        # We have to sort the batch by their non-padded lengths in descending order,\n",
    "        # because the descending order can help in `nn.utils.rnn.pack_padded_sequence()`,\n",
    "        # which it will help us ignoring the <pad> in training rnn.\n",
    "        # https://meetonfriday.com/posts/4d6a906a\n",
    "        src_batch, trg_batch = zip(\n",
    "            *sorted(\n",
    "                zip(src_batch, trg_batch),\n",
    "                key=lambda x: sum(x[0].attention_mask),\n",
    "                reverse=True,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return src_batch, trg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = SentencePieceDataModule(\r\n",
    "    data_dir,\r\n",
    "    ch_tokenizer_dir,\r\n",
    "    jp_tokenizer_dir,\r\n",
    "    config[\"batch_size\"],\r\n",
    "    config[\"num_workers\"],\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dm.setup(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000 32000\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "input_dim = dm.src_tokenizer.get_vocab_size()\n",
    "output_dim = dm.trg_tokenizer.get_vocab_size()\n",
    "print(input_dim, output_dim)\n",
    "\n",
    "src_pad_idx = dm.src_tokenizer.token_to_id(\"[PAD]\")\n",
    "print(src_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 Encoding(num_tokens=105, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "64 Encoding(num_tokens=117, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "for src, trg in dm.test_dataloader():\r\n",
    "    print(len(src), src[0])\r\n",
    "    print(len(trg), trg[0])\r\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Lightning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "![](../assets/bi_encoder.png)\n",
    "\n",
    "é¦–å…ˆæˆ‘å€‘å…ˆè¼¸å…¥ embeded éå¾Œçš„å­—ä¾†è¨ˆç®—æ­£å‘å’Œåå‘çš„ hidden state:\n",
    "\n",
    "$$\n",
    "h_\\overrightarrow{t} = \\overrightarrow{\\text{EncoderGRU}}(\\text{emb}(x_\\overrightarrow{t}), h_\\overrightarrow{t-1})\n",
    "\\\\\n",
    "h_\\overleftarrow{t} = \\overleftarrow{\\text{EncoderGRU}}(\\text{emb}(x_\\overleftarrow{t}), h_\\overleftarrow{t-1})\n",
    "$$\n",
    "\n",
    "å¾—åˆ°çš„ `outputs` ä»£è¡¨æ‰€æœ‰æœ€å¾Œä¸€å±¤çš„ hidden states çš„çµ„åˆï¼Œæˆ‘å€‘æœƒç”¨ outputs ä¾†è¨ˆç®— attentionï¼Œä¹Ÿå°±æ˜¯ç¿»è­¯æ™‚è¦æ³¨æ„åŸå¥çš„å“ªäº›å–®å­—:\n",
    "\n",
    "$$\n",
    "h_1 = [h_\\overrightarrow{1}; h_\\overleftarrow{1}], h_2 = [h_\\overrightarrow{2}; h_\\overleftarrow{2}], \\\\\n",
    "\\text{outputs} = H = \\left\\{h_1, h_2, \\cdots, h_T\\right\\}\n",
    "$$\n",
    "\n",
    "å¾—åˆ°çš„ `hidden` ä»£è¡¨æ¯ä¸€å±¤æœ€å¾Œä¸€å€‹æ™‚é–“é»çš„ hidden states çš„ç–ŠåŠ ï¼Œæˆ‘å€‘æœƒç”¨ hidden åšç‚º decoder åˆå§‹çš„ context vector `s0`:\n",
    "\n",
    "$$\n",
    "\\overrightarrow{z} = h_\\overrightarrow{T} \\\\\n",
    "\\overleftarrow{z} = h_\\overleftarrow{T}\n",
    "$$\n",
    "\n",
    "å› ç‚º decoder ä¸æ˜¯é›™å‘ï¼Œæ‰€ä»¥æˆ‘å€‘æŠŠ hidden ä¸Ÿé€²ä¸€å€‹ linear `g` å’Œ `tanh` è£¡ç²å¾—æ¿ƒç¸®å¾Œçš„ context vector `z`:\n",
    "\n",
    "$$\n",
    "z = \\tanh(g(\\text{cat}(\\overrightarrow{z}, \\overleftarrow{z}))) = s_0\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "| variables   | use                                             | note                                              |\n",
    "| ----------- | ----------------------------------------------- | ------------------------------------------------- |\n",
    "| src         | åˆå§‹èªè¨€è³‡æ–™                                    | `shape=[batch_size, src_len]` (batch-first shape) |\n",
    "| src_len     | batch ä¸­æ¯å€‹å¥å­çš„çœŸå¯¦é•·åº¦                      | `shape=[batch_size]`                              |\n",
    "| input_dim   | åˆå§‹èªè¨€çš„ vocab_size                           | `src_tokenizer.get_vocab_size()`                  |\n",
    "| emb_dim     | embedding_size                                  ||\n",
    "| enc_hid_dim | Encoder ä¸­ rnn çš„ hidden_size                   ||\n",
    "| dec_hid_dim | Decoder ä¸­ rnn çš„ hidden_size                   ||\n",
    "| rnn         | é›™å‘ GRU, åƒ batch-first çš„è³‡æ–™                 | `nn.GRU(emb_dim, enc_hid_dim, bidirectional = True, batch_first=True)`|\n",
    "| fc          | å°‡é›™å‘ context vector è¼¸å‡ºæˆå–®å€‹ context vector | `nn.Linear(enc_hid_dim * 2, dec_hid_dim)`|\n",
    "\n",
    "### Outputs\n",
    "\n",
    "| variables        | use                                            | note                                     |\n",
    "| ---------------- | ---------------------------------------------- | ---------------------------------------- |\n",
    "| packed_embedded  | å°‡ `[PAD]` åˆªæ‰åŒ…è£æˆ packed æ ¼å¼              | `PackedSequence`                         |\n",
    "| packed_outputs   | æ²’æœ‰ `[PAD]` çš„æœ€å¾Œä¸€å±¤ hidden_states          | `PackedSequence`                         |\n",
    "| enc_outputs      | æœ‰ `[PAD]` çš„æœ€å¾Œä¸€å±¤ hidden_states            | `shape=[batch_size, src_len, enc_hid_dim*2]` |\n",
    "| hidden           | æ‰€æœ‰ layer ç–ŠåŠ çš„ context_vector               | `shape=[layer*2, batch_size, enc_hid_dim]`   |\n",
    "| hidden[:2, :, :] | æœ€ä¸Šé¢ forward layer çš„ hidden_state           | `shape=[batch_size, enc_hid_dim]`            |\n",
    "| hidden[:1, :, :] | æœ€ä¸Šé¢ backward layer çš„ hidden_state          | `shape=[batch_size, enc_hid_dim]`            |\n",
    "| last_hidden      | é€é torch.cat çµ„åˆæœ€å¾Œä¸€å±¤ forward + backward | `shape=[batch_size, enc_hid_dim*2]`          |\n",
    "| dec_hidden       | ç¶“é tanh + fc å¾—åˆ°çš„ context_vector           | `shape=[batch_size, dec_hid_dim]`\n",
    "\n",
    "> - **Terminology Alert**ğŸ˜ª:\n",
    ">   - **outputs** å¯ä»¥æƒ³æˆæ‰€æœ‰æ™‚é–“é»çš„æœ€å¾Œä¸€å±¤çš„ hidden_states æ‰€çµ„æˆ\n",
    ">   - **hidden** å¯ä»¥æƒ³æˆæ‰€æœ‰ layer (forward + backward) åœ¨æœ€å¾Œæ™‚é–“é»çš„ hidden_states å †ç–Šè€Œæˆçš„ context_vector\n",
    "\n",
    "å•é¡Œä¸€: ä»€éº¼æ˜¯ packed_sequence?\n",
    "> - [[Pytorch]Pack the data to train variable length sequences](https://meetonfriday.com/posts/4d6a906a)\n",
    "\n",
    "å•é¡ŒäºŒ: outputs å’Œ hidden å·®åœ¨å“ª?\n",
    "> - [å­¦ä¼šåŒºåˆ† RNN çš„ output å’Œ state](https://zhuanlan.zhihu.com/p/28919765)\n",
    "> - [LSTM/GRUä¸­outputå’Œhiddençš„åŒºåˆ«//å…¶ä»–é—®é¢˜](https://blog.csdn.net/yagreenhand/article/details/84893493)\n",
    "\n",
    "<img src=\"../assets/output_vs_hidden.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_len):\n",
    "        # src     = [batch_size, src_len]\n",
    "        # src_len = [batch_size]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to(\"cpu\"), batch_first=True)\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "        # packed_outputs is a packed sequence containing all hidden states\n",
    "        # hidden is now from the final non-padded element in the batch\n",
    "\n",
    "        enc_outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "        # enc_outputs is now a non-packed sequence\n",
    "\n",
    "        # enc_outputs = [batch_size, src_len, enc_hid_dim*num_directions]\n",
    "        #             = [forward_n + backward_n]\n",
    "        #             = [last layer]\n",
    "\n",
    "        # hidden  = [n_layers*num_directions, batch_size, enc_hid_dim]\n",
    "        #         = [forward_1, backward_1, forward_2, backword_2, ...]\n",
    "\n",
    "        # hidden[-2, :, : ] is the last of the forwards RNN\n",
    "        # hidden[-1, :, : ] is the last of the backwards RNN\n",
    "\n",
    "        last_hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        init_dec_hidden = torch.tanh(self.fc(last_hidden))\n",
    "\n",
    "        # enc_outputs     = [batch_size, src_len, enc_hid_dim*2]  (we only have 1 layer)\n",
    "        # init_dec_hidden = [batch_size, dec_hid_dim]\n",
    "\n",
    "        return enc_outputs, init_dec_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](../assets/seq2seq_encoder_attention.png)\n",
    "\n",
    "å…ˆèªªçµè«–ï¼Œæ¯æ¬¡ attention layer æœƒç”¢ç”Ÿä¸€å€‹ `src_len` é•·åº¦çš„é™£åˆ—ï¼Œä»£è¡¨åœ¨é æ¸¬ä¸‹ä¸€å€‹å­— $\\hat{y}_{t+1}$ çš„æ™‚é–“é»æ™‚ï¼Œå°åŸå¥ `src` ä¸­æ¯ä¸€å€‹ token çš„å°ˆæ³¨åº¦æœ‰å¤šé«˜ã€‚\n",
    "\n",
    "ğŸ¤¯ æ¯æ¬¡éœ€è¦çµ¦ attention layer ä»€éº¼?\n",
    "\n",
    "1. decoder å‰ä¸€å€‹æ™‚é–“é»çš„ hidden state $s_{t-1}$ (i.e., ç¬¬ä¸€å€‹å°±æ˜¯ encoder çš„ `hidden` $z$ ä¹Ÿå°±æ˜¯ $s_0$)\n",
    "2. encoder çš„ `outputs` $H$\n",
    "\n",
    "è€Œ attention layer å…¶å¯¦åªæ˜¯ä¸€å€‹ linear layerï¼Œç”¨ä¾†å’Œ `tanh` ä¸€èµ·è¨ˆç®—å‡ºä¸€å€‹èƒ½é‡å€¼ $E_t$:\n",
    "\n",
    "$$\n",
    "E_t = \\tanh(\\text{attn}(s_{t-1}, H))\n",
    "$$\n",
    "\n",
    "å› ç‚º `enc_outputs` çš„é•·åº¦æ˜¯ `src_len`ï¼Œè€Œ `hidden` åªæ˜¯ä¸€å€‹ scalarï¼Œæ‰€ä»¥æˆ‘å€‘å¿…é ˆæŠŠ `hidden` æ‹‰åˆ°è·Ÿ `enc_outputs` ä¸€æ¨£é•·ã€‚ è¨ˆç®—å‡ºä¾†çš„ $E_t$ å¯ä»¥æƒ³åƒæˆ `encoder_outputs` $H$ å’Œ `previous_decoder_hidden_state` $s_{t-1}$ æœ‰å¤šåŒ¹é…ã€‚\n",
    "\n",
    "å› ç‚ºç®—å‡ºä¾†çš„èƒ½é‡å€¼ $E_t$ å½¢ç‹€æ˜¯ `[src_len, hid_dim]`ï¼Œæˆ‘å€‘å¯ä»¥æŠŠä»–å¸¶å…¥ä¸€å€‹å½¢ç‹€æ˜¯ `[hid_dim, 1]` linear layer $v$ã€‚æœ€çµ‚å¾—åˆ°ä¸€å€‹å½¢ç‹€æ˜¯ `[src_len]` çš„ `attention_sequence`:\n",
    "\n",
    "$$\n",
    "\\hat{a}_t = v(E_t)\n",
    "$$\n",
    "\n",
    "ä½ å¯ä»¥æƒ³åƒ $v$ è£¡é¢å­¸ç¿’åˆ°çš„åƒæ•¸æ˜¯ä¸€å€‹æ¬Šé‡ï¼Œå‘Šè¨´æˆ‘å€‘èƒ½é‡å€¼ $E_t$ ä½œç”¨åœ¨ `encoder_outputs` ä¸­æ¯å€‹ token çš„æ¬Šé‡æœ‰å¤šå°‘ã€‚\n",
    "\n",
    "æœ€å¾Œçš„æœ€å¾Œï¼Œ attention_sequence æœƒé€šé softmax è®“æ‰€æœ‰æ©Ÿç‡åŠ ç¸½ç‚º 1ï¼Œå…¶ä¸­æœƒæŠŠ `attention_sequence` å’Œ `mask` çµåˆï¼Œè®“å°æ‡‰åœ¨ [PAD] index çš„ hidden state éƒ½è®Šæˆ -1e10 (æœƒè®“ä»–å€‘åœ¨å¥—å…¥ softmax å¾Œè®Šæˆ 0)ã€‚\n",
    "\n",
    "$$\n",
    "a_t = \\text{softmax}(\\hat{a}_t)\n",
    "$$\n",
    "\n",
    "é€™å€‹ $a_t$ æ­£æ˜¯å‘Šè¨´æˆ‘å€‘åœ¨ decode çš„ç•¶ä¸‹ï¼Œè¦æ³¨è¦–åŸå¥çš„å“ªäº› token!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "| variable        | use                                                   | note                                          |\n",
    "| --------------- | ----------------------------------------------------- | --------------------------------------------- |\n",
    "| hidden          | encoder_hidden $s_0$ æˆ–æ˜¯ decoder_hidden $s_{t-1}$     | `shape=[batch_size, dec_hid_dim]`              |\n",
    "| encoder_outputs | encoder_outputs $H$ï¼Œä¹Ÿå°±æ˜¯ encoder æœ€å¾Œä¸€å±¤çš„ hidden_states   | `shape=[batch_size, src_len, enc_hid_dim * 2]` |\n",
    "| mask            | ç”¨ä¾†é®ç½©çš„ tensorï¼Œ1 æ˜¯çœŸå¯¦çš„ tokenï¼Œ0 æ˜¯ [PAD]            | `shape=[batch_size, src_len]`      |\n",
    "| attn            | ç”¨ä¾†åŒ¹é… `enc_outputs` å’Œ `hidden` çš„ attention layer | `linear(enc_hid*2+dec_hid, dec_hid)` |\n",
    "| v               | ç”¨ä¾†å­¸ç¿’ `attention` æ¬Šé‡çš„ linear layer              | `linear(dec_hid, 1)`                 |\n",
    "\n",
    "### Outputs\n",
    "\n",
    "| variable              | use                                                                                                       | note                                       |\n",
    "| --------------------- | --------------------------------------------------------------------------------------------------------- | ------------------------------------------ |\n",
    "| energy                | è¨ˆç®— attention sequence çš„ç¬¬ä¸€å€‹ç”¢ç‰©ï¼Œå°‡ hidden+encoder_outputs çµ„åˆå¾Œï¼Œé€é attention_layer å’Œ tanh ç®—å‡º | `shape=[batch_size, src_len, dec_hid_dim]` |\n",
    "| attention             | å°‡èƒ½é‡å€¼ `energy` ä¸Ÿå…¥ `v` ä¸­å­¸ç¿’æ¬Šé‡å¾Œç”¢ç”Ÿçš„ attention sequenceï¼Œä½†é‚„æ²’è™•ç† padding                      | `shape=[batch_size, src_len]`              |\n",
    "| attention.masked_fill | æŠŠ attention sequence ç•¶ä¸­ï¼Œindex æ˜¯ [PAD] çš„åœ°æ–¹æ”¹æˆ -1e10ï¼Œè®“ä»–å€‘é€šé softmax éƒ½æœƒè®Šæˆ 0    | `tensor.masked_fill(mask, dim=n)` |                                                                                                         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "\n",
    "        # hidden = [batch_size, dec_hid_dim]\n",
    "        # encoder_outputs = [batch_size, src_len, enc_hid_dim * 2]\n",
    "\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        # hidden = [batch_size, 1, dec_hid_dim]        (unsqueeze 1)\n",
    "        #        = [batch_size, src_len, dec_hid_dim]  (repeat)\n",
    "        \n",
    "        stacked_hidden = torch.cat((hidden, encoder_outputs), dim=2)\n",
    "        # stacked_hidden = [batch_size, src_len, dec_hid_dim + enc_hid_dim * 2]\n",
    "\n",
    "        energy = torch.tanh(self.attn(stacked_hidden))\n",
    "        # energy = [batch_size, src_len, dec_hid_dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        # attention = [batch_size, src_len, 1]   (v)\n",
    "        #           = [batch_size, src_len]      (squeeze)\n",
    "\n",
    "        attention = attention.masked_fill(mask == 0, -(2**15))\n",
    "\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "![](../assets/seq2seq_decoder_attention.png)\n",
    "\n",
    "Attention æ©Ÿåˆ¶æœƒç”¨å‰ä¸€å€‹æ™‚é–“é» $t-1$ çš„ `hidden` $s_{t-1}$ å’Œä»£è¡¨æ•´å€‹åŸå¥çš„ `encoder_outputs` $H$ï¼Œè¨ˆç®—å‡ºç¾åœ¨æ™‚é–“é» $t$ çš„ attention vector $a_t$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E_t &= \\tanh(\\text{attn}(s_{t-1}, H)) \\\\\n",
    "\\hat{a}_t &= vE_t \\\\\n",
    "a_t &= \\text{softmax}(\\hat{a}_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "æ¥è‘—ï¼Œæˆ‘å€‘å†ä½¿ç”¨ $a_t$ å° $H$ é€²è¡Œ `matrix-matrix product`ï¼Œæ‰¾å‡ºçœŸæ­£èƒ½è¡¨é”åŸå¥ä¸­ï¼Œå°æ¯å€‹ token å°ˆæ³¨åŠ›çš„ `weighted_sum` $w_t$ã€‚\n",
    "\n",
    "$$\n",
    "w_t = a_tH\n",
    "$$\n",
    "\n",
    "æ¥è‘—å°±å¯ä»¥é€šé `decoderGRU` è¨ˆç®—ç¾åœ¨æ™‚é–“é» $t$ çš„ `hidden` $s_t$:\n",
    "\n",
    "$$\n",
    "s_t = \\text{DecoderGRU}(d(y_t), w_t, s_{t-1})\n",
    "$$\n",
    "\n",
    "ææ–™æœ‰:\n",
    "\n",
    "1. embedded token $d(y_t)$ (ä¾‹åœ–ä¸­ decoder çš„ `<sos>` ç¶“é embedding çš„çµæœ)\n",
    "2. ä¸Šé¢ç®—å‡ºä¾†çš„ weighted source vector $w_t$\n",
    "3. å‰ä¸€å€‹æ™‚é–“é»çš„ decoder çš„ `hidden` $s_{t-1}$\n",
    "\n",
    "é æ¸¬ä¸‹ä¸€å€‹ token $\\hat{y}_{t+1}$ å°±å¾ˆç°¡å–®äº†ï¼Œåªè¦æŠŠæ±è¥¿éƒ½å‚™é½Šï¼Œæ”¾é€² linear layer `fc_out` å°±å¥½:\n",
    "\n",
    "$$\n",
    "\\hat{y}_{t+1} = f(d(y_t), w_t, s_t)\n",
    "$$\n",
    "\n",
    "### Inputs\n",
    "\n",
    "| variable        | use                                                | note                                                        |\n",
    "| --------------- | -------------------------------------------------- | ----------------------------------------------------------- |\n",
    "| inp           | åœ¨ç¾åœ¨æ™‚é–“é» $t$ æ™‚è¼¸å…¥åˆ° decoder çš„ token         | `shape=[batch_size]`                                        |\n",
    "| hidden          | å‰ä¸€å€‹æ™‚é–“é» $t-1$ çš„ hidden state                 | `shape=[batch_size, dec_hid_dim]`                           |\n",
    "| encoder_outputs | Encoder æœ€å¾Œä¸€å±¤çš„ hidden_states $H$               | `shape=[batch_size, src_len, enc_hid_dim*2]`                |\n",
    "| mask            | çµ¦ attention ç”¨ä¾†ç„¡è¦– `[PAD]` çš„ 0/1s              | `shape=[batch_size, src_len]`                               |\n",
    "| output_dim      | ç›®æ¨™èªè¨€çš„ `vocab_size`ï¼Œç”¨ä¾†ç•¶ embedding è¼¸å‡ºå¤§å° | `trg_tokenizer.get_vocab_size()`                            |\n",
    "| rnn             | å–®å±¤ä¸”å–®å‘çš„ GRUï¼Œåƒ batch_first çš„è³‡æ–™            | `GRU(enc_hid_dim*2+emb_dim, dec_hid_dim, batch_first=True)` |\n",
    "| fc_out          | é æ¸¬ä¸‹ä¸€å€‹ token çš„ linear layer                   | `Linear(enc_hid_dim*2+dec_hid_dim+emb_dim, output_dim)`     |\n",
    "\n",
    "### Outputs\n",
    "\n",
    "| variable   | use                                                  | note                                          |\n",
    "| ---------- | ---------------------------------------------------- | --------------------------------------------- |\n",
    "| a          | attention vector                                     | `shape=[batch_size, src_len]`                 |\n",
    "| embedded   | input token ç¶“é embedding å¾—åˆ°çš„çµæœ                | `shape=[batch_size, emb_dim]`                 |\n",
    "| weighted   | attention å’Œ encoder_outputs ä¹˜ç©å¾—åˆ°çš„ weighted sum | `shape=[batch_size, src_len]`                 |\n",
    "| rnn_input  | embedded å’Œ weighted å †ç–Š                            | `shape=[batch_size, enc_hid_dim*2 + emb_dim]` |\n",
    "| output     | DecoderGRU çš„ hidden state $s_t$                     | `shape=[batch_size, dec_hid_dim]`             |\n",
    "| hidden     | DecoderGRU çš„ hidden state $s_t$                     | `shape=[batch_size, dec_hid_dim]`             |\n",
    "| prediction | é æ¸¬ä¸‹ä¸€å€‹ token æ˜¯å­—å…¸ä¸­å“ªä¸€å€‹ token çš„æ©Ÿç‡åˆ†å¸ƒ     | `shape=[batch_size, output_dim]`              |\n",
    "\n",
    "> 1. forward ä¸­å¾ˆå¤šå‘é‡éƒ½æ“´å……äº†ä¸€å€‹ç¶­åº¦ï¼Œé‚£æ˜¯ä»£è¡¨ seq_len=1\n",
    "> 2. å› ç‚º decoderGRU åªæœ‰å–®å±¤ã€å–®æ™‚é–“é»ï¼Œæ‰€ä»¥ output å’Œ hidden æ˜¯ä¸€æ¨£çš„æ±è¥¿ï¼éƒ½æ˜¯ $s_t$\n",
    "> 3. `torch.bmm` æ˜¯ç°¡å–®çš„çŸ©é™£ç›¸ä¹˜\n",
    ">     1. ä¸€å®šè¦ 3 ç¶­çŸ©é™£\n",
    ">     2. å…¬å¼æ˜¯ $b\\times n\\times m @ b\\times m\\times p = b\\times n\\times p$\n",
    ">     3. `bmm((10, 3, 4), (10, 4, 5)) = (10, 3, 5)`\n",
    ">     4. [documentation](https://pytorch.org/docs/stable/generated/torch.bmm.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inp, hidden, encoder_outputs, mask):\n",
    "        # encoder_outputs = [batch_size, src_len, enc_hid_dim*2]\n",
    "        # hidden = [batch_size, dec_hid_dim]\n",
    "\n",
    "        inp = inp.unsqueeze(1)\n",
    "        # inp = [batch_size]\n",
    "        #     = [batch_size, 1]  (unsqueeze 1)\n",
    "\n",
    "        # embedded = [batch_size, 1, emb_dim]\n",
    "        embedded = self.dropout(self.embedding(inp))\n",
    "        \n",
    "        # a = [batch_size, src_len]\n",
    "        #   = [batch_size, 1, src_len]  (unsqueeze 1)\n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        # weighted = [batch_size, 1, enc_hid_dim*2]\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "\n",
    "        # rnn_input = [batch_size, 1, emb_dim + enc_hid_dim*2]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        \n",
    "        # hidden = [1, batch_size, dec_hid_dim]  (unsqueeze 0)\n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        \n",
    "        # output = [batch_size, 1, dec_hid_dim]\n",
    "        # hidden = [1, batch_size, dec_hid_dim]\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        # embedded = [batch_size, emb_dim]        (squeeze 0)\n",
    "        # output   = [batch_size, dec_hid_dim]    (squeeze 0)\n",
    "        # weighted = [batch_size, enc_hid_dim*2]  (squeeze 0)\n",
    "        # hidden = [batch_size, dec_hid_dim]      (squeeze 0)\n",
    "        embedded = embedded.squeeze(1)\n",
    "        output = output.squeeze(1)\n",
    "        weighted = weighted.squeeze(1)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        \n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        predict_input = torch.cat((output, weighted, embedded), dim=1)\n",
    "\n",
    "        # prediction = [batch_size, output_dim]\n",
    "        prediction = self.fc_out(predict_input)\n",
    "\n",
    "        # a = [batch_size, src_len]  (squeeze 1)\n",
    "        a = a.squeeze(1)\n",
    "\n",
    "        return prediction, hidden, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "æˆ‘ç”¨ `pl.LightningModule` ä¾†å°è£æ‰€æœ‰ seq2seq æ¨¡å‹çš„ training å’Œ validation (ä½¿ç”¨è‡ªå·±çš„ `_forward()` å‡½å¼)ã€ä»¥åŠ test step (ä½¿ç”¨å…§å»ºçš„ `forward()` å‡½å¼)ã€‚è¼¸å…¥çš„ `config` ç‚ºç¶²è·¯ä¸­æ‰€æœ‰å¯ä»¥è¢«èª¿æ•´çš„è¶…åƒæ•¸ï¼Œå¯ä»¥ç”¨æ–¼åŸ·è¡Œ `wandb sweep` (hyperparameter tuning)ã€‚\n",
    "\n",
    "### Training\n",
    "\n",
    "åœ¨ seq2seq ä¸­ï¼Œè¨“ç·´æ™‚ (`_forward()`) é¦–å…ˆå¾ encoder ç²å¾—å…©ç¨® final_hidden_states (åˆ†åˆ¥æ˜¯ outputs å’Œ hidden):\n",
    "\n",
    "1. outputs: ç”±æ¯å€‹æ™‚é–“é»çš„ final_linear è¼¸å‡ºçš„ hidden_states ç–ŠåŠ è€Œæˆï¼Œä½œç‚º attention ç”¨é€”\n",
    "2. hidden: ç”±æœ€å¾Œä¸€å€‹æ™‚é–“é»çš„æ‰€æœ‰ hidden_states çµ„åˆè€Œæˆï¼Œä½œç‚ºåˆå§‹çš„ decoder_hidden_states\n",
    "\n",
    "å†ä¾†å°±æ˜¯ decoder è¨“ç·´çš„éƒ¨åˆ†:\n",
    "\n",
    "- `preds` ç”¨ä¾†å„²å­˜æ‰€æœ‰é æ¸¬ $\\hat{y}$ çš„çµæœ\n",
    "- å°‡æ‰€æœ‰ (ä¸€å€‹ batch) è¦æ”¾å…¥ decoder çš„ input_tokens éƒ½è¨­ç‚º `[BOS]`\n",
    "- åœ¨ loop è£¡é¢é€²è¡Œ decode:\n",
    "    - å¾€ decoder ä¸Ÿå…¥ input_token $y_t$ å’Œå‰ä¸€å€‹ hidden_state $s_{t-1}$ åŠ encoder_outputs $H$\n",
    "    - ç²å¾—é æ¸¬å€¼ $\\hat{y}_{t+1}$ å’Œæ–°çš„ hidden_state $s_t$\n",
    "    - æ©Ÿç‡æ€§ä½¿ç”¨ `teacher_force`:\n",
    "        - ä½¿ç”¨: ä¸‹ä¸€æ¬¡çš„ input_token æ˜¯ ground_truth\n",
    "        - ä¸ä½¿ç”¨: ä¸‹ä¸€æ¬¡çš„ input_token å°±æ˜¯æœ¬æ¬¡é æ¸¬ $\\hat{y}_{t+1}$\n",
    "\n",
    "decode çš„é †åºæ˜¯å¾ 1 é–‹å§‹ï¼Œé€™æ˜¯ç‚ºäº†è®“ `preds` èƒ½å¤ è·Ÿ target å°ç¨±ï¼Œç•¶æˆ‘å€‘è¦è¨ˆç®— loss æ™‚ï¼Œå†æŠŠ target å’Œ `preds` çš„ç¬¬ä¸€å€‹ç æ‰å°±å¥½:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{trg} &= \\begin{bmatrix} &\\text{[BOS]}, &y_1, &y_2, &y_3, &\\text{[EOS]} \\end{bmatrix} \\\\\n",
    "\\text{preds} &= \\begin{bmatrix} &&&0, &\\hat{y}_1, &\\hat{y}_2, &\\hat{y}_3, &\\text{[EOS]} \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "### Inference\n",
    "\n",
    "åœ¨åš inference (`forward()`) çš„æ™‚å€™ï¼Œé™¤äº†ä¸æœƒç”¨ `teacher_force` å¤–ï¼Œæˆ‘å€‘çš„ decode loop æœƒå¾ 1 è·‘åˆ°è‡ªå®šç¾©çš„ `max_len`ï¼Œè®“ decode åŸ·è¡Œåˆ° `max_len` çµæŸç‚ºæ­¢ã€‚æˆ‘æœƒåœ¨å…¨éƒ¨ batch éƒ½é æ¸¬å®Œæˆå¾Œï¼Œå†ä¾†åˆ‡æ‰ä»»ä½•å¥å­å‡ºç¾ `[EOS]` ä¹‹å¾Œçš„ tokensã€‚\n",
    "\n",
    "``` python\n",
    "eos_pos = dict((preds == self.trg_tokenizer.token_to_id(\"[EOS]\")).nonzero().tolist())\n",
    "\n",
    "real_sentence = sentence[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None]\n",
    "real_attention = attention[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None, :src_len]\n",
    "```\n",
    "\n",
    "å¦å¤–åœ¨ inference æœƒåŒæ™‚è¨˜éŒ„ attention_matrix ä½œç‚º case study ç”¨é€”ã€‚åŸ·è¡Œå®Œæ‰€æœ‰çš„é æ¸¬å¾Œï¼Œæœƒå°æ¯ä¸€å€‹è¦é æ¸¬çš„å¥å­å›å‚³å››å€‹ç‰©ä»¶ (`test_outputs`):\n",
    "\n",
    "|variable|shape|desc|\n",
    "|-|-|-|\n",
    "|pred_sentence | [trg_len] | é æ¸¬çš„å¥å­ tokens |\n",
    "|attn_matrix   | [trg_len, src_len] | pred_sentence å’Œ src_sentence çš„å°ˆæ³¨åŠ›çŸ©é™£ |\n",
    "|src_sentence  | [src_len] | åŸå¥ tokens |\n",
    "|trg_sentence  | [trg_len] | ç›®æ¨™å¥ tokens |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel(pl.LightningModule):\r\n",
    "    def __init__(self, input_dim, output_dim, src_tokenizer, trg_tokenizer, config):\r\n",
    "        super().__init__()\r\n",
    "        self.src_pad_idx = src_tokenizer.token_to_id(\"[PAD]\")\r\n",
    "        self.trg_tokenizer = trg_tokenizer\r\n",
    "        self.input_dim = input_dim\r\n",
    "        self.output_dim = output_dim\r\n",
    "\r\n",
    "        self.encoder = Encoder(\r\n",
    "            input_dim,\r\n",
    "            config[\"enc_emb_dim\"],\r\n",
    "            config[\"enc_hid_dim\"],\r\n",
    "            config[\"dec_hid_dim\"],\r\n",
    "            config[\"enc_dropout\"],\r\n",
    "        )\r\n",
    "\r\n",
    "        attn = Attention(config[\"enc_hid_dim\"], config[\"dec_hid_dim\"])\r\n",
    "\r\n",
    "        self.decoder = Decoder(\r\n",
    "            output_dim,\r\n",
    "            config[\"dec_emb_dim\"],\r\n",
    "            config[\"enc_hid_dim\"],\r\n",
    "            config[\"dec_hid_dim\"],\r\n",
    "            config[\"dec_dropout\"],\r\n",
    "            attn,\r\n",
    "        )\r\n",
    "\r\n",
    "        self.lr = config[\"lr\"]\r\n",
    "        self.apply(self.init_weights)\r\n",
    "        self.save_hyperparameters()\r\n",
    "    \r\n",
    "    \r\n",
    "    def init_weights(self, m):\r\n",
    "        for name, param in m.named_parameters():\r\n",
    "            if 'weight' in name:\r\n",
    "                nn.init.normal_(param.data, mean=0, std=0.01)\r\n",
    "            else:\r\n",
    "                nn.init.constant_(param.data, 0)\r\n",
    "    \r\n",
    "    \r\n",
    "    # Training\r\n",
    "    # Use only when training and validation\r\n",
    "    def _forward(self, src, trg, teacher_forcing_ratio=0.5):\r\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\r\n",
    "        # e.g., if teacher_forcing_ratio is 0.5 we use teacher forcing 50% of the time\r\n",
    "\r\n",
    "        # src = list of Encoding([ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\r\n",
    "        # trg = list of Encoding([ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\r\n",
    "\r\n",
    "        # src_batch = [batch_size, src_len]\r\n",
    "        # src_mask  = [batch_size, src_len]\r\n",
    "        # src_len   = [batch_size]\r\n",
    "        src_batch = torch.tensor([e.ids for e in src], device=self.device)\r\n",
    "        src_mask = torch.tensor([e.attention_mask for e in src], device=self.device)\r\n",
    "        src_len = torch.sum(src_mask, axis=1)\r\n",
    "\r\n",
    "        # trg_batch = [batch_size, trg_len]\r\n",
    "        trg_batch = torch.tensor([e.ids for e in trg], device=self.device)\r\n",
    "\r\n",
    "        batch_size = src_batch.shape[0]\r\n",
    "        trg_len = trg_batch.shape[1]\r\n",
    "        trg_vocab_size = self.output_dim\r\n",
    "\r\n",
    "        # create a tensor for storing all decoder outputs\r\n",
    "        preds = torch.zeros(batch_size, trg_len, trg_vocab_size, device=self.device)\r\n",
    "\r\n",
    "        # encoder_outputs is all hidden states of the input sequence, back and forwards\r\n",
    "        # hidden is the final forward and backward hidden states, passed through a linear layer\r\n",
    "        encoder_outputs, hidden = self.encoder(src_batch, src_len)\r\n",
    "        \r\n",
    "        # first input to the decoder = [BOS] tokens\r\n",
    "        # inp = [batch_size]\r\n",
    "        inp = trg_batch[:, 0]\r\n",
    "\r\n",
    "        for t in range(1, trg_len):\r\n",
    "            # pred   = [batch_size, output_dim]\r\n",
    "            # hidden = [batch_size, dec_hid_dim]\r\n",
    "            pred, hidden, _ = self.decoder(inp, hidden, encoder_outputs, src_mask)\r\n",
    "\r\n",
    "            # store predictions in a tensor holding predictions for each token\r\n",
    "            preds[:, t, :] = pred\r\n",
    "            \r\n",
    "            # decide if we are going to use teacher forcing or not\r\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\r\n",
    "\r\n",
    "            # top1 = [batch_size]\r\n",
    "            # get the highest predicted token from our predictions\r\n",
    "            top1 = pred.argmax(1)\r\n",
    "\r\n",
    "            # inp = [batch_size]\r\n",
    "            # if teacher forcing, use actual next token as next input\r\n",
    "            # if not, use predicted token\r\n",
    "            inp = trg_batch[:, t] if teacher_force else top1\r\n",
    "        \r\n",
    "        return preds\r\n",
    "    \r\n",
    "    \r\n",
    "    # Inference\r\n",
    "    # * Let you use the pl model as a pytorch model.\r\n",
    "    # * \r\n",
    "    # * pl_model.eval()\r\n",
    "    # * pl_model(X)\r\n",
    "    # *\r\n",
    "    def forward(self, src, max_len=100):\r\n",
    "        src_batch = torch.tensor([e.ids for e in src], device=self.device)\r\n",
    "        src_mask = torch.tensor([e.attention_mask for e in src], device=self.device)\r\n",
    "        src_len = torch.sum(src_mask, axis=1)  # actual src_len without [PAD]\r\n",
    "        \r\n",
    "        batch_size = src_batch.shape[0]\r\n",
    "        src_size = src_batch.shape[1]  # src_len with [PAD]\r\n",
    "        trg_len = max_len\r\n",
    "        trg_vocab_size = self.output_dim\r\n",
    "        \r\n",
    "        preds = torch.zeros(batch_size, trg_len, trg_vocab_size, device=self.device)\r\n",
    "        encoder_outputs, hidden = self.encoder(src_batch, src_len)\r\n",
    "        \r\n",
    "        # create a tensor for storing all attention matrices\r\n",
    "        attns = torch.zeros(batch_size, trg_len, src_size, device=self.device)\r\n",
    "        \r\n",
    "        # first input to the decoder = [BOS] tokens\r\n",
    "        # inp = [batch_size]\r\n",
    "        inp = torch.tensor([self.trg_tokenizer.token_to_id(\"[BOS]\")], device=self.device).repeat(batch_size)\r\n",
    "        \r\n",
    "        for t in range(1, trg_len):\r\n",
    "            \r\n",
    "            # attn = [batch_size, src_len]\r\n",
    "            pred, hidden, attn = self.decoder(inp, hidden, encoder_outputs, src_mask)\r\n",
    "            \r\n",
    "            preds[:, t, :] = pred\r\n",
    "            top1 = pred.argmax(1)\r\n",
    "            inp = top1\r\n",
    "            \r\n",
    "            # store attention sequences in a tensor holding attention value for each token\r\n",
    "            attns[:, t, :] = attn\r\n",
    "            \r\n",
    "        return preds, attns, src_len\r\n",
    "\r\n",
    "\r\n",
    "    def training_step(self, batch, batch_idx):\r\n",
    "        # both are lists of encodings\r\n",
    "        src, trg = batch\r\n",
    "        \r\n",
    "        # y    = [batch_size, trg_len]\r\n",
    "        # pred = [batch_size, trg_len, output_dim]\r\n",
    "        y = torch.tensor([e.ids for e in trg], device=self.device)\r\n",
    "        preds = self._forward(src, trg)\r\n",
    "        output_dim = preds.shape[-1]\r\n",
    "        \r\n",
    "        # y    = [batch_size * (trg_len-1)]\r\n",
    "        # pred = [batch_size * (trg_len-1), output_dim]\r\n",
    "        y = y[:, 1:].reshape(-1)\r\n",
    "        preds = preds[:, 1:, :].reshape(-1, output_dim)\r\n",
    "        \r\n",
    "        loss = F.cross_entropy(preds, y, ignore_index=src_pad_idx)\r\n",
    "        self.log(\"train_loss\", loss)\r\n",
    "\r\n",
    "        perplexity = torch.exp(loss)\r\n",
    "        self.log(\"train_ppl\", perplexity)\r\n",
    "        \r\n",
    "        if self.global_step % 50 == 0:\r\n",
    "            torch.cuda.empty_cache()\r\n",
    "            \r\n",
    "        return loss\r\n",
    "    \r\n",
    "    \r\n",
    "    def validation_step(self, batch, batch_idx):\r\n",
    "        src, trg = batch\r\n",
    "        y = torch.tensor([e.ids for e in trg], device=self.device)\r\n",
    "        preds = self._forward(src, trg)\r\n",
    "        \r\n",
    "        output_dim = preds.shape[-1]\r\n",
    "        y = y[:, 1:].reshape(-1)\r\n",
    "        preds = preds[:, 1:, :].reshape(-1, output_dim)\r\n",
    "        \r\n",
    "        loss = F.cross_entropy(preds, y, ignore_index=src_pad_idx)\r\n",
    "        self.log(\"valid_loss\", loss, sync_dist=True)\r\n",
    "        \r\n",
    "        perplexity = torch.exp(loss)\r\n",
    "        self.log(\"valid_ppl\", perplexity, sync_dist=True)\r\n",
    "        \r\n",
    "        \r\n",
    "    def test_step(self, batch, batch_idx):\r\n",
    "        src, trg = batch\r\n",
    "        preds, attn_matrix, real_src_len = self(src)\r\n",
    "        \r\n",
    "        # attn_matrix = [batch_size, trg_len, src_len]\r\n",
    "        # preds       = [batch_size, trg_len, output_dim]\r\n",
    "        #             = [batch_size, trg_len]             (argmax 2)\r\n",
    "        preds = preds.argmax(2)\r\n",
    "        \r\n",
    "        # convert `preds` tensor to list of real sentences (tokens)\r\n",
    "        # meaning to cut the sentence by [EOS] and remove the [PAD] tokens\r\n",
    "        \r\n",
    "        # eos_pos = dict(sentence_idx: first_pad_position)\r\n",
    "        #\r\n",
    "        # e.g., {0: 32, 2: 55} \r\n",
    "        # Meaning that we have 32 tokens (include [EOS]) in the first predicted sentence\r\n",
    "        # and `max_len` tokens (no [EOS]) in the second predicted setence\r\n",
    "        # and 55 tokens (include [EOS]) in the third predicted sentence\r\n",
    "        eos_pos = dict(reversed((preds == self.trg_tokenizer.token_to_id(\"[EOS]\")).nonzero().tolist()))\r\n",
    "    \r\n",
    "        pred_sentences, attn_matrices = [], []\r\n",
    "        for idx, (sentence, attention, src_len) in enumerate(zip(preds, attn_matrix, real_src_len)):\r\n",
    "            \r\n",
    "            # sentence  = [trg_len_with_pad]\r\n",
    "            #           = [real_trg_len]\r\n",
    "            pred_sentences.append(sentence[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None])\r\n",
    "            \r\n",
    "            # attention = [trg_len_with_pad, src_len_with_pad]\r\n",
    "            #           = [real_trg_len, real_src_len]\r\n",
    "            attn_matrices.append(attention[:eos_pos.get(idx)+1 if eos_pos.get(idx) else None, :src_len])\r\n",
    "        \r\n",
    "        # source sentences for displaying attention matrix \r\n",
    "        src = [[token for token in e.tokens if token != \"[PAD]\"] for e in src]\r\n",
    "        \r\n",
    "        # target sentences for calculating BLEU scores\r\n",
    "        trg = [[token for token in e.tokens if token != \"[PAD]\"] for e in trg]\r\n",
    "        \r\n",
    "        return pred_sentences, attn_matrices, src, trg\r\n",
    "        \r\n",
    "    \r\n",
    "    def test_epoch_end(self, test_outputs):\r\n",
    "        outputs = []\r\n",
    "        for (pred_sent_list, attn_list, src_list, trg_list) in test_outputs:\r\n",
    "            for pred_sent, attn, src, trg in list(zip(pred_sent_list, attn_list, src_list, trg_list)):\r\n",
    "                pred_sent = list(map(self.trg_tokenizer.id_to_token, pred_sent))\r\n",
    "                outputs.append((pred_sent, attn, src, trg))\r\n",
    "        \r\n",
    "        # outputs = list of predictions of testsets, each has a tuple of (pred_sentence, attn_matrix, src_sentence, trg_sentence)\r\n",
    "        # pred_sentence = [trg_len]\r\n",
    "        # attn_matrix   = [trg_len, src_len]\r\n",
    "        # src_sentence  = [src_len]\r\n",
    "        # trg_sentence  = [trg_len]\r\n",
    "        self.test_outputs = outputs\r\n",
    "        \r\n",
    "    def configure_optimizers(self):\r\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\r\n",
    "    \r\n",
    "    \r\n",
    "    def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):\r\n",
    "        optimizer.zero_grad(set_to_none=True)\r\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = pl.loggers.WandbLogger()\n",
    "\n",
    "model = Seq2SeqModel(\n",
    "    input_dim,\n",
    "    output_dim,\n",
    "    dm.src_tokenizer,\n",
    "    dm.trg_tokenizer,\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 84,620,032 trainable parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": "Seq2SeqModel(\n  (encoder): Encoder(\n    (embedding): Embedding(32000, 300)\n    (rnn): GRU(300, 512, batch_first=True, bidirectional=True)\n    (fc): Linear(in_features=1024, out_features=512, bias=True)\n    (dropout): Dropout(p=0.3, inplace=False)\n  )\n  (decoder): Decoder(\n    (attention): Attention(\n      (attn): Linear(in_features=1536, out_features=512, bias=True)\n      (v): Linear(in_features=512, out_features=1, bias=False)\n    )\n    (embedding): Embedding(32000, 300)\n    (rnn): GRU(1324, 512, batch_first=True)\n    (fc_out): Linear(in_features=1836, out_features=32000, bias=True)\n    (dropout): Dropout(p=0.3, inplace=False)\n  )\n)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/windsuzu/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: Checkpoint directory checkpoints exists and is not empty.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = Path(\"checkpoints\")\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(dirpath=ckpt_dir,  # path for saving checkpoints\n",
    "                                          filename=\"{epoch}-{valid_loss:.3f}\",\n",
    "                                          monitor=\"valid_loss\",\n",
    "                                          mode=\"min\",\n",
    "                                          save_top_k=3,\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    fast_dev_run=False,\n",
    "    logger=wandb_logger,\n",
    "    gpus=1,\n",
    "    max_epochs=5,\n",
    "    gradient_clip_val=1,\n",
    "    precision=config[\"precision\"],\n",
    "    callbacks=[checkpoint],\n",
    "    # resume_from_checkpoint=ckpt_dir / \"epoch=4-valid_loss=3.802.ckpt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing (BLEU Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqModel.load_from_checkpoint(ckpt_dir / checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449289d6b39e454f8430093ee08b51a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=â€¦"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_corpus_bleu(preds: List[str], refs: List[List[str]], n_gram=4):\r\n",
    "    # arg example:\r\n",
    "    # preds: [\"æœºå™¨äººè¡Œä¸šåœ¨ç¯å¢ƒé—®é¢˜ä¸Šçš„æªæ–½\", \"æ¾ä¸‹ç”Ÿäº§ç§‘æŠ€å…¬å¸ä¹Ÿä»¥ç¯å¢ƒå…ˆè¿›ä¼ä¸šä¸ºç›®æ ‡\"]\r\n",
    "    # refs: [[\"æœºå™¨äººåœ¨ç¯å¢ƒä¸Šçš„æ”¹å˜\", \"å°æ–¼æœºå™¨äººåœ¨ç¯å¢ƒä¸Šçš„æªæ–½\"],  [\"æ¾ä¸‹ç§‘æŠ€å…¬å¸çš„é¦–è¦ç›®æ ‡æ˜¯è§£å†³ç¯å¢ƒé—®é¢˜\"]]\r\n",
    "    preds = list(map(list, preds))\r\n",
    "    refs = [[list(sen) for sen in ref] for ref in refs]\r\n",
    "    return torchmetrics.functional.nlp.bleu_score(preds, refs, n_gram=n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{}]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4356)\n"
     ]
    }
   ],
   "source": [
    "preds = [dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, output[0]))) for output in model.test_outputs]\n",
    "refs = [[dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, output[3])))] for output in model.test_outputs]\n",
    "\n",
    "bleu_score = calculate_corpus_bleu(preds, refs)\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study and Attention Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif'] = ['Noto Sans CJK TC']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def case_study(pred_token, src_token, trg_token, attn_matrix):\n",
    "    \n",
    "    src  = dm.src_tokenizer.decode(list(map(dm.src_tokenizer.token_to_id, src_token)))\n",
    "    trg  = dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, trg_token)))\n",
    "    pred = dm.trg_tokenizer.decode(list(map(dm.trg_tokenizer.token_to_id, pred_token)))\n",
    "    \n",
    "    print(f\"SOURCE: \\n{src}\\n {'-'*100}\")\n",
    "    print(f\"TARGET: \\n{trg}\\n {'-'*100}\")\n",
    "    print(f\"PREDICTION: \\n{pred}\\n {'-'*100}\")\n",
    "    \n",
    "    print(f\"BLEU SCORE: {calculate_corpus_bleu([trg], [[pred]])}\")\n",
    "    \n",
    "    plt.figure(figsize=(30, 30))\n",
    "    ax = sns.heatmap(attn_matrix, xticklabels=src_token, yticklabels=pred_token)\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: \n",
      "åœ¨ 2001 å¹´çš„FAO / WHOé£Ÿå“æ·»åŠ å‰‚è”åˆå§”å‘˜ä¼š ( JECFA ) æŠ¥å‘Šäº† 3 - MCPDä»¥åŠ 1 , 3 - DCPçš„æ¯’æ€§ ã€‚\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "TARGET: \n",
      "2001 å¹´ã®FAO / WHOåˆåŒé£Ÿå“æ·»åŠ ç‰©å°‚é–€å®¶å§”å“¡ä¼šä¼šè­° ( JECFA ) ã«ã¦ 3 âˆ’ MCPDãŠã‚ˆã³ 1 , 3 âˆ’ DCPã®æ¯’æ€§ãŒå ±å‘Šã•ã‚Œã¦ã„ã‚‹ ã€‚\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "PREDICTION: \n",
      "2001 å¹´ã®FAO / WHOåˆåŒé£Ÿå“æ·»åŠ å§”å“¡ä¼š ( JECFA ) ã«ã¯ , 3 âˆ’ MCPDãŠã‚ˆã³ 1 , 3 âˆ’ DCPã®æ¯’æ€§æ€§ã«ã¤ã„ã¦å ±å‘Šã—ãŸ ã€‚\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "BLEU SCORE: 0.7500602006912231\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 2160x2160 with 2 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i=13\n",
    "case_study(model.test_outputs[i][0],\n",
    "           model.test_outputs[i][2],\n",
    "           model.test_outputs[i][3],\n",
    "           model.test_outputs[i][1].cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd079b9df49ac430b00ede86aa066ef9244a5bf89bf4dad1c74865db678bf985bb6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}