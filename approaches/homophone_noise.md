# Robust Neural Machine Translation with Joint Textual and Phonetic Embedding

Reference: https://arxiv.org/pdf/1810.06729.pdf

## Motivation

Neural machine translation (NMT) is notoriously sensitive to noises, but noises are almost inevitable in practice. One special kind
of noise is the homophone noise, where words are replaced by other words with similar pronunciations.

Despite tremendous success, NMT models are very sensitive to the noises in input sentences (Belinkov and Bisk, 2017).  It is very common for a user to accidentally choose a homophone instead of the correct word.

The transformer model can correctly translate the clean input sentence; however, when one Mandarin character, â€˜æœ‰â€™, is replaced by one of its homophones, â€˜åˆâ€™, the transformer generates a strange and irrelevant translation. 

![](../assets/homophone_noise.png)

For homophone noises, since correct phonetic information exists, we can make use of it to make the output of the embedding layer
much more robust.

> å› ç‚º embedding å¯ä»¥è¦–ç‚ºç¶²è·¯ä¸­æœ€æ—©çš„è¼¸å…¥ï¼Œæ‰€ä»¥éŒ¯å­—çš„ semantic embedding å°‡æœƒç›´æ¥å¸¶åæ•´å€‹ç¥ç¶“ç¶²è·¯çš„è¨“ç·´ã€‚ è€Œ phonetic embedding æ°å¥½å¯ä»¥ä½œç‚ºæŠ—è¡¡ semantic embedding ç™¼ç”ŸéŒ¯èª¤çš„ç‰¹å¾µã€‚
> - semantic å› éŒ¯å­—è¢«å¸¶å
> - phonetic ä¸å› éŒ¯å­—è¢«å¸¶å

## Joint Embedding

å®šç¾© $a$ ç‚º**ä¾†æºå–®å­—** (source word)ï¼Œå¯ä»¥è¢«æ‹†æˆ**å¤šå€‹ç™¼éŸ³å…ƒä»¶** $s_i$ (pronounciation units, e.g., phonemes or syllables)ï¼Œå¯«ä½œ:

$$
\Psi(a) = \left\{ s_1, s_2, \cdots, s_n\right\}
$$

è€Œ $a$ çš„ embedding å®šç¾©ç‚º $\pi(a)$ï¼Œç„¶å¾Œ $s$ çš„ embedding å®šç¾©ç‚º $\pi(s)$ï¼Œæ–¼æ˜¯æˆ‘å€‘ä¸€å…±æœƒå¾—åˆ° $n+1$ å€‹ embedding vector:

$$
\pi(a), \pi(s_1), \pi(s_2), \cdots, \pi(s_n)
$$

æˆ‘å€‘æœƒå°‡æ‰€æœ‰çš„ç™¼éŸ³å…ƒä»¶ embedding $\pi(s_1), \cdots, \pi(s_n)$ åˆ©ç”¨**å¹³å‡**å¾—åˆ°å–®ä¸€çš„ embedding vectorï¼Œè¨˜ä½œ $\pi(\Psi(a))$ï¼›æœ€å¾Œå°‡ä»–å’Œ word embedding $\pi(a)$ åˆä½µèµ·ä¾†:

$$
\pi([a, \Psi(a)]) = (1- \beta) \times \pi(a) + \beta \times\pi(\Psi(a))
$$

è£¡é¢çš„ $\beta$ æ˜¯ä¸€å€‹è¶…åƒæ•¸ï¼Œç•¶ $\beta=0$ æ™‚ï¼Œåªä½¿ç”¨ word embeddingï¼›ç•¶ $\beta=1$ æ™‚ï¼Œåªä½¿ç”¨ phonetic embeddingã€‚

---

> Github å¥½è®€ç‰ˆ ğŸ‘
> 
> ![](../assets/joint_embedding.png)

## Model

- Pytorch 0.4.0 OpenNMT
- 8 GPUs
- 6 layers
- 8 heads attention
- 2048 neurons in feed-forward layer
- 512 neurons in other layers
- dropout=0.1
- label smoothing 0.1
- Adam
- learning rate=2 with NOAM decay

## Observation

1. phonetic embedding ä¸Šç›¸è¿‘çš„ç™¼éŸ³æœƒèšé›†åœ¨ä¸€èµ·

![](../assets/pronounced_embedding_visualization.png)

2. phonetic embedding èƒ½æœ‰æ•ˆæå‡ non-noise datasetï¼Œä¹Ÿèƒ½å¹«åŠ© noisy dataset

![](../assets/phonetics_against_homophone_noises.png)

3. beta=0.95 æ™‚æ•ˆæœæœ€å¥½ï¼Œä¹Ÿå°±æ˜¯ 0.05 semantic embedding + 0.95 phonetic embedding

![](../assets/semantic_phonetic_beta.png)

4. Case Study

![](../assets/homophone_improvements.png)

## Related Work

1. Formiga and Fonollosa (2012) proposed to use a character-level translator to deal with misspelled words in the input sentences, but in general their method cannot deal with homophone noises effectively.
2. Cheng et al. (2018) proposed to use adversarial stability training to improve the robustness of NMT systems, but their method does not specifically target homophone noises and do not use phonetic information.
3. Li et al. (2018) also proposed to utilize both textual and phonetic information to improve the robustness of NMT systems, but their method is different with ours in how textual and phonetic information are combined.